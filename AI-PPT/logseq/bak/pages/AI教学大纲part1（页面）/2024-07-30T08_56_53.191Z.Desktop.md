# **《2024 AI全局洞察与快速落地》   **
- 第一大部分（从大语言模型LLM到通用人工智能AGI（看清站位））
	- 第一点（本课价值）
	- 第二点（生成式AI简史）
	- 第三点（大语言模型LLM（三项技术突破））
	- 第四点（AGI（通用人工智能））
	- 第五点（LLM天花板）
	- 第六点（生成式时代人工智能发展的特点）
	- 第七点（大模型的底座作用及AI对各行业的影响）
- 第二大部分（国内外AI现状与大众困惑（认清全局））**
	- ### **第一点** ** 全球：百家争鸣** 
	  
	  OpenAI Google Meta  Anthropic Nvidia Mistral HuggingFace Microsoft...
	- ### **第** **二** **点** ** ** **国内：群雄争霸** 
	  
	  百度文心一言 阿里通义千言 腾讯元宝 天工AI 抖音豆包...
	- ### **第** **三** **点** ** ** **普通人：如何借力Ai 祝我一臂之力（需求）**
	- ### **第** **四** **点** ** ** **最大问题：陷入局部而不自知；**
	- ### **第** **五** **点** ** ** **困惑点&坑点（如何选择？如何避坑？如何用好？）**
	-
- 第三大部分（看清全局并快速上手的最佳策略）**
	- ### **第一点** ** ** **国内外众多模型评测** 
	  
	  著名的llama家族
	- ### **第** **二** **点** ** ** **国内外众多应用（根据流量访问，已经圈定范围，依然乱花渐欲迷人眼）**
	- ### **第** **三** **点** ** ** **到底如何选择（亲测+诸多权威人士推荐/评测 并行）** 
	  
	  **第****四****点 ****如何现场测试（提示词的写法+要点；结合自己掌握的知识库的测评问题设置）**
- ## **第** **四** **大部分（** **现场测评：侧重推理和搜索能力** **）**
	- ### **第** **一** **点** ** 经典问题**
	  #标签测试
	- **第****二****点 ****现实问题---prompt调优**
	- ### 第三点 实时问题
	- ### **第** **四** **点 复杂问题**
	- ### **第** **五** **点 专业问题**
- ## **第** **五** **大部分（** **Embedding** **）**
	- ### **第一点** **（** **向量化/** **Embedding** ** 概念）** 
	  id:: 66a731ed-3184-4336-8d71-07adea75f7da
	  
	  **向量化**
	  
	  **在深度学习中，向量化通常指的是将输入数据（如文本、图像等）转换为固定大小的向量表示，这些向量可以进一步用于模型的训练和推理。**
	  
	  **特点：**
	  
	  <!--[if !supportLists]-->**· **<!--[endif]-->**高效性：****通过同时对多个数据元素执行相同的操作，显著提高计算效率。**
	  
	  <!--[if !supportLists]-->**· **<!--[endif]-->**简化数据处理：****将复杂的数据结构简化为易于处理的向量形式。**
		- #标签测试
		  ---
		  
		  **Embedding**
		  
		  **Embedding是一种将离散数据（如单词、类别标签等）转换为连续向量表示的技术。在自然语言处理中，Embedding通过将每个单词映射到一个高维空间中的点（即向量），使得具有相似含义的单词在向量空间中彼此接近。这样，模型就可以在向量空间中进行数学运算，如计算向量之间的距离或相似度，从而理解单词之间的关系。** 
		  
		  **特点：**
		  
		  <!--[if !supportLists]-->**· **<!--[endif]-->**语****义信息保留：**Embedding向量能够捕捉单词的语义信息，使得模型能够理解单词之间的复杂关系。
		  
		  <!--[if !supportLists]-->**· **<!--[endif]-->**灵活性**：可以根据不同的任务和数据集学习得到不同的Embedding表示。
		  
		  ---
		  
		  **Embedding技术：**
		  
		  **Embedding技术就像是这个字典的现代版，但它不是简单地给每个单词一个编号，而是给每个单词一个复杂的数字“指纹”。 这个“指纹”是一个****由很多数字组成的向量****，就像是一串数字序列。这个序列能够捕捉到单词的很多特性：**
		  
		  **比如它的意思、它在句子中的作用，甚至是它的情感色彩。如果我们有“快乐”和“悲伤”这两个词，Embedding技术会生成两个不同的向量。尽管这两个词在字典里可能紧挨着，但它们的向量会相差很远，因为它们表达的情感是相反的，计算机可以通过比较这两个向量的距离，来理解这两个词在情感上的不同。**
		  
		  ---
		  
		  **Embedding 层**
		  
		  E**mbedding 层是神经网络中的一个基础组件，它将整数索引（代表离散的类别）映射到固定大小的密集向量。每个整数索引对应一个唯一的向量，这些向量通常在训练过程中学习得到。在 NLP 中，Embedding 层通常用于将单词（通过其索引表示）映射到向量，这些向量可以****捕捉到单词的语义信息****。**
		  
		  **在机器学习和深度学习中，嵌入层（embedding layer）是一种特殊的神经网络层，它的作用是将离散的输入数据（如单词、物品、用户ID等）映射到连续的向量表示。**
		  
		  **这种映射是通过一个可训练的权重矩阵实现的，该矩阵的行数等于输入数据的类别数（例如，词汇表中的单词数），列数等于嵌入向量的维度。**
		  
		  ---
		  
		  **例如，假设我们有一个包含 10,000 个单词的词汇表，我们想要将每个单词映射到一个 128 维的向量。那么，Embedding 层将会有 10,000 个输入（每个单词一个）和 128 个输出（每个单词的向量表示）。**
		  
		  **在 PyTorch 中，Embedding 层的基本用法如下：**
		  
		  **import torch.nn as nn**
		  
		  **假设词汇表大小为 10,000，嵌入向量的维度为 128**
		  
		  **embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=128)**
		  
		  **embedding的输入时是整数张量，每个整数代表一个词汇的索引，输出是一个浮点数张量，每个浮点数都代表着对应词汇的词嵌入向量。**
		  
		  **tips：****0阶张量表示的是一个标量，1阶张量表示的是一个向量，2阶张量表示的是一个矩阵**
			-
		- ### **第** **二** **点** **（** **向量化/** **Embedding** ** 图示 ）** 
		  
		  如下图所示，文本向量化模型通过将“生活日常知识”转换为数值向量，可以将文本信息表示成能够表达文本语义的向量
		- ### **第** **三** **点** **（** **Embedding 层的** **作用和语义理解）**
		- ### **嵌入层的作用** 
		  
		  <!--[if !supportLists]-->**1. **<!--[endif]-->**降维：**
		  
		  **在自然语言处理和机器学习中：**
		  
		  **高维空间****通常指的是包含****大量特征或属性的数据集****所在的空间。这些特征或属性可以是数值型的，也可以是分类型的，它们共同描述了数据点的各个方面。**
		  
		  **低维空间****则是指维度较低的空间，即****包含较少特征或属性的数据集所在的空间****。在低维空间中，数据点可以通过较少的参数来描述，这通常使得数据的可视化和分析变得更加容易。**
		  
		  **降维的过程****就是将数据点从高维空间转换到低维空间的过程，同时尽可能保持****数据点****之间的****相对距离和结构****不变。（即：尽可能保留数据的关键信息）**
		  
		  **在自然语言处理中，****降维通常通过嵌入层****来实现，如Word2Vec、GloVe或Transformer模型中的嵌入层，将高维的one-hot编码转换为低维的连续向量，这些向量能够捕获单词间的语义和语法关系，从而在低维空间中表示文本数据。**
		  
		  **降维的目的****是在****减少数据维度的同时，尽可能保留数据的关键特征****。**
		  
		  **最终目的****是将高维的文本向量压缩到低维空间中，是为了能够在低维空间中更有效地分析和处理数据，****以便于分析和理解。****例如通过****可视化****来发现模式、通过简化模型来****减少计算成本****、或通过压缩数据来****减少存储需求****。**
		  
		  ** **
		  
		  ---
		  
		  **嵌入层中的降维确实可以被视为一种在保持语义或关键特征前提下的数据压缩技术。不仅减少了数据的维度，而且还保留了数据的重要结构和信息**
			-
		- ### **保持语义或关键特征** 
		  
		  <!--[if !supportLists]-->**· **<!--[endif]-->**语义保持：****嵌入层通过将高维的one-hot编码转换为低维的连续向量，单词的向量表示仍然能够反映出它们在语义上的相似性或差异**
		  
		  <!--[if !supportLists]-->**· **<!--[endif]-->**关键特征保留：****在降维过程中，嵌入层通过学习，能够保留和浓缩数据中的关键特征。例如，一个单词的嵌入向量可以捕获到其在不同上下文中的使用频率、语法角色和语义含义，这些都是理解文本数据的关键特征。**
		- ### **变相压缩技术** 
		  
		  <!--[if !supportLists]-->**· **<!--[endif]-->**数据压缩**：**从信息论的角度来看，嵌入层的降维过程可以视为一种数据压缩技术。通过将****高维的离散****表示转换为****低维的连续****表示，不仅减少了数据存储和处理所需的资源，还通过学习得到了更加紧凑和高效的数据表示。**
		  
		  <!--[if !supportLists]-->**· **<!--[endif]-->**效率与性能**：**这种压缩技术在提高计算效率和模型性能方面发挥了重要作用。低维的嵌入向量使得模型在进行****计算时更加高效****，同时也能够捕获到****数据的深层结构****，从而在各种NLP任务中表现出色。**
		  
		  ---
		  
		  
		  
		  <!--[if !supportLists]-->**2. **<!--[endif]-->**泛化：嵌入层可以学习到输入数据的分布式表示，这种表示能够****捕捉到数据中的语义或结构信息****，有助于模型****泛化到未见过的数据****。**
		- ### <!--[if !supportLists]--> **3. ** <!--[endif]--> **相似性：在嵌入空间中，** **语义上相似或相关的输入数据** **（如单词的同义词、物品的类似类别等）会被映射到相近的向量，这使得模型能够利用这些** **相似性** **。**
		- ### <!--[if !supportLists]--> **4. ** <!--[endif]--> **效率：相比于使用one-hot编码，嵌入向量提供了更加** **紧凑** **的数据表示，可以减少模型的参数数量，提高训练和推理的效率。** 
		  
		  **嵌入层被定义为网络的第一个隐藏层**  
		  
		  
		  
		  ---
		  
		  **Embedding Dimensions（嵌入尺寸）**
		  
		  **所谓的“低纬”指的是：Embedding Dimensions（嵌入尺寸）；指在一个嵌入模型中，用于表示每一个独特输入（如单词、句子或实体）的****向量维度****数。换句话说，它定义了每个嵌入向量在向量空间中的长度或宽度，即有多少个数值组成这个向量。**
		  
		  **增加嵌入维度能够****为模型提供更丰富的表示能力****，允许模型学习到****更复杂的特征和关系****，但也相应地****增加了模型的复杂性和计算成本****。因此，选择合适的嵌入维度是设计嵌入模型时的一个重要决策点。**
		  
		  **语义表示：****更高纬的Embedding Dimensions****能够为输入提供****更丰富、更细致的语义****表示。这意味着模型能够学习到更复杂的上下文关系和特征，**
		- ---
		  
		  **Model Size**
		  
		  **向量模型大小（参数数量）"Model Size (Million Parameters) 通常指的是在向量空间中表示模型所需的参数总量。 **
		  
		  **即：Model Size=词汇表大小（num_embeddings）✖每个嵌入向量的维度（embedding_dim）**
		  
		  <!--[if !supportLists]-->**l **<!--[endif]-->**num_embeddings 表示词汇表的大小，即模型能够区分的不同类别或单词的数量。**
		  
		  <!--[if !supportLists]-->**l **<!--[endif]-->**embedding_dim 是每个单词或类别对应的嵌入向量的维度，越多使模型越能捕获更复杂的语义特征**
-
-