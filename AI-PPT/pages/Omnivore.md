## 🔖 文章
	- [Omnivore 入门 --- Getting Started with Omnivore](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2)
	  collapsed:: true
	  site:: [Omnivore](https://omnivore.app/zhangxinxin0216/getting-started-with-omnivore)
	  author:: unknown
	  labels:: [[插件类]]
	  date-saved:: [[Aug 23rd, 2024]]
		- ### 高亮
		  collapsed:: true
			- > * Saving 储蓄
			  * Reading 读数
			  * Organizing 组织
			  * Integrations 集成 [⤴️](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2#ce4b3b05-d311-4b9e-bdcc-462d6d7496e6)
			- > **图书馆**是您 Omnivore 体验的中心 [⤴️](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2#843cb26a-b673-4a42-afe2-7341c64a2343)
			- > 从 Mac 存储 PDF [⤴️](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2#81e8fe5b-a27c-4e36-a2cb-7900a36c1731)
	- [The Secret Power of ‘Read It Later’ Apps（《哈佛商业评论》最近发表的一篇文章为这种新的神经现象命名：注意力缺陷特质）](https://omnivore.app/me/the-secret-power-of-read-it-later-apps)
	  collapsed:: true
	  site:: [Forte Labs](https://fortelabs.co/blog/the-secret-power-of-read-it-later-apps)
	  author:: Tiago Forte
	  labels:: [[插件类]]
	  date-saved:: [[Aug 21st, 2024]]
	  date-published:: [[Jan 24th, 2022]]
		- ### 高亮
		  collapsed:: true
			- > 安静地坐着并持续引导集中注意力 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#577a975d-2a51-4b1b-a893-c61a00af33e7)
			- > 变得极其稀缺 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#e00eb005-313c-46a5-9e06-252e66166db0)
			- > 阅读能力正在成为世界竞争优势的源泉 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#fb149552-6d21-4a21-ba8a-49f7f4a89337)
			- > 注意力 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#685878d3-c8d7-4f52-a0c9-16bfdac4ee12)
			- > ：注意力缺陷特质 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#ac3cc662-2b2f-4ddd-a8bb-bcd2108e298a)
			- > 《哈佛商业评论》 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#1f16f434-4adb-49c7-a772-8556d1c95c23)
			- > 分心、内心狂热和不耐烦 [⤴️](https://omnivore.app/me/the-secret-power-of-read-it-later-apps#70ed8da2-246a-4e77-9a0b-8f7312a3ee5e)
	- [文心一言](https://omnivore.app/me/-1917df2e673)
	  collapsed:: true
	  site:: [yiyan.baidu.com](https://yiyan.baidu.com/chat/4524638783)
	  author:: unknown
	  labels:: [[文心一言]]
	  date-saved:: [[Aug 23rd, 2024]]
		- ### 高亮
		  collapsed:: true
			- > API Endpoint还可以与安全性措施和权限控制机制相结合，以确保只有具有适当权限的客户端才能访问特定的资源。这有助于保护API的敏感数据和功能不被未经授权的访问。 [⤴️](https://omnivore.app/me/-1917df2e673#39f3afa6-e654-4144-ac0f-e7b529fbfbd2)
	- [开源RAG框架汇总-CSDN博客](https://omnivore.app/me/rag-csdn-1917dcec1e0) 
	  site:: [blog.csdn.net](https://blog.csdn.net/qq_33137873/article/details/138683590?depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%257Edefault%257EBlogCommendFromBaidu%257ERate-1-138683590-blog-141191700.235%255Ev43%255Epc_blog_bottom_relevance_base9&spm=1001.2101.3001.6650.1)
	  author:: 成就一亿技术人!
	  labels:: [[CSDN]] [[RAG]]
	  date-saved:: [[Aug 23rd, 2024]]
	  date-published:: [[May 18th, 2024]]
		- ### 高亮
		  collapsed:: true
			- > FastGPT [⤴️](https://omnivore.app/me/rag-csdn-1917dcec1e0#bf4a66fa-a853-47e9-ab6c-463eecb239b7)
			- > QAnything [⤴️](https://omnivore.app/me/rag-csdn-1917dcec1e0#88f30683-b772-4470-8478-7a4154721127)
			- > open-webui [⤴️](https://omnivore.app/me/rag-csdn-1917dcec1e0#bef7bed8-0d53-4e52-affe-85ab9a72f42f)
			- > RAGFlow [⤴️](https://omnivore.app/me/rag-csdn-1917dcec1e0#a9da2136-4c30-4cee-9fd0-046ef37b670a)
	- [Qwen2 -微调 Qwen2_qwen2 微调-CSDN博客](https://omnivore.app/me/qwen-2-qwen-2-qwen-2-csdn-1917d392cab)
	  collapsed:: true
	  site:: [blog.csdn.net](https://blog.csdn.net/sinat_37574187/article/details/140603260?biz_id=0&ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522172438273216800185884936%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fall.%252522%25257D&request_id=172438273216800185884936&spm=1018.2226.3001.4187)
	  author:: 成就一亿技术人!
	  labels:: [[CSDN]] [[大模型/微调]]
	  date-saved:: [[Aug 23rd, 2024]]
	  date-published:: [[Jul 22nd, 2024]]
		- [![丹努什·库马尔](https://img-blog.csdnimg.cn/img_convert/6eea6312bd3a8f0afa43f01afed5362e.jpeg)](https://medium.com/@danushidk507?source=post%5Fpage-----f89c5c9d15da--------------------------------)
		- 阅读时间：7 分钟 2024 年 6 月 8 日
		- 阿里云最新系列语言模型由于性能提升和安全功能增强，于周五推出后迅速跃居开源 LLM 排行榜首位。
		- Qwen2 系列包括各种基础语言模型和指令调整语言模型，大小从 0.5 到 720 亿个参数，以及混合专家 (MoE) 模型。
		- 这些更新的功能使其在协作人工智能平台 Hugging Face 的 Open LLM Leaderboard 上占据首位，可用于商业或研究活动。
		- ![](https://img-blog.csdnimg.cn/img_convert/32c4169b9b8d467745a2224cab6def6a.jpeg)
		- 中国电商巨头阿里巴巴在中国人工智能领域占有重要地位。今天，阿里巴巴发布了最新的人工智能模型 Qwen2，该模型被认为是目前最好的开源模型之一。
		- Qwen2 由阿里云开发，代表了该公司统一千文（Qwen）模型系列的下一阶段，该系列包括统一千文 LLM（Qwen）、视觉 AI 模型 Qwen-VL 和 Qwen-Audio。
		- Qwen 模型系列已针对不同行业和领域的多语言数据进行了预训练，其中 Qwen-72B 是该系列中最强大的模型。该模型已针对惊人的 3 万亿个 token 数据进行了训练。相比之下，Meta 最强大的 Llama-2 变体基于 2 万亿个 token 构建。与此同时，Llama-3 目前正在处理 15 万亿个 token。
		- ## 模型详细信息
		- Qwen2 是一系列语言模型，包括各种大小的解码器模型。阿里巴巴已针对每种大小发布了基础语言模型和对齐聊天模型。这些模型基于 Transformer 架构构建，具有 SwiGLU 激活、注意 QKV 偏差、组查询注意、滑动窗口注意和全注意的混合等功能。此外，阿里巴巴还开发了一种增强的标记器，可适应多种自然语言和代码。
		- * 型号尺寸：Qwen2–0.5B、Qwen2–1.5B、Qwen2–7B、Qwen2–57B-A14B 和 Qwen2–72B；
		- * 除了英语和中文之外，还使用了 27 种语言的数据进行训练；
		- * 各项基准测试中取得优异成绩；
		- * 增强编码和数学能力；
		- * 使用 Qwen2-7B 将上下文长度支持扩展到 128K 个标记
		- * Instruct 和 Qwen2–72B-Instruct。
		- ## 模型信息
		- Qwen2 系列由 5 种尺寸的基础和指令调整模型组成：Qwen2–0.5B、Qwen2–1.5B、Qwen2–7B、Qwen2–57B-A14B 和 Qwen2–72B。下表详细介绍了这些模型的基本信息：
		- ![](https://img-blog.csdnimg.cn/img_convert/50414b4d6b2131aa17754e73a855bf5a.png)
		- ## 表现
		- 对比评估显示，大规模模型（70B+参数）相比Qwen1.5有显著的性能提升。本研究重点评估大规模模型Qwen2–72B的性能，从自然语言理解、知识获取、编码能力、数学能力、多语言能力等多个方面对Qwen2–72B与前沿开放模型进行比较。得益于精心挑选的数据集和精湛的训练技巧，Qwen2–72B在与Llama-3–70B等顶级模型的对决中展现出优异的表现，尤其在参数较少的情况下，其性能表现优于上一代Qwen1.5–110B。
		- ![](https://img-blog.csdnimg.cn/img_convert/4a150140159a51d02bc3e2beadfddbfb.jpeg)
		- ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">从 transformers 导入 AutoModelForCausalLM、AutoTokenizer
		  - device = <span style="color:#c41a16">"cuda" </span> <span style="color:#007400"># 将模型加载到的设备</span>
		  - model = AutoModelForCausalLM.from_pretrained( <span style="color:#c41a16">"Qwen/Qwen1.5-7B-Chat"</span> , device_map= <span style="color:#c41a16">"auto"</span> )
		  - tokenizer = AutoTokenizer.from_pretrained( <span style="color:#c41a16">"Qwen/Qwen1.5-7B-Chat"</span> )
		  - prompt = <span style="color:#c41a16">"请简单介绍一下大型语言模型。"</span>
		  - 消息 = [{ <span style="color:#c41a16">“角色”</span>：<span style="color:#c41a16">“用户”</span>，<span style="color:#c41a16">“内容”</span>：提示}]
		  - 文本 = tokenizer.apply_chat_template（消息，tokenize = False，add_generation_prompt = True）
		  - model_inputs = tokenizer（[text]，return_tensors = <span style="color:#c41a16">“pt”</span>）。到（设备）
		  - generated_ids = model.generate（model_inputs.input_ids，max_new_tokens = 512，do_sample = True）
		  - generated_ids = [output_ids [len（input_ids）：] for input_ids，output_ids in zip（model_inputs.input_ids，generated_ids）]
		  - 响应 = tokenizer.batch_decode（generated_ids，skip_special_tokens = True）[0]</span></span>
		  - ```
		  - ### Qwen2Config
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">（vocab_size = <span style="color:#1c00cf">151936</span> hidden_​​size = <span style="color:#1c00cf">4096i</span> ntermied_size = <span style="color:#1c00cf">22016</span> num_hidden_​​layers = <span style="color:#1c00cf">32</span> num_attention_heads = <span style="color:#1c00cf">32</span> num_key_value_heads = <span style="color:#1c00cf">32</span> hidden_​​act = <span style="color:#c41a16">'silu'</span> max_position_embeddings = <span style="color:#1c00cf">32768i</span> nitializer_range = <span style="color:#1c00cf">0.02</span> rms_norm_eps = <span style="color:#1c00cf">1e-06</span> use_cache = Truetie_word_embeddings = Falserope_theta = <span style="color:#1c00cf">10000.0</span> use_sliding_window = Falsesliding_window = <span style="color:#1c00cf">4096</span> max_window_layers = <span style="color:#1c00cf">28tention_dropout</span> = <span style="color:#1c00cf">0.0</span> **kwargs）</span></span>
		  - ```
		  - **参数**
		  - * vocab\_size（`int`，_可选，默认为 151936）— Qwen2 模型的词汇量。定义调用_[Qwen2Model](https://huggingface.co/docs/transformers/v4.41.3/en/model%5Fdoc/qwen2#transformers.Qwen2Model)`inputs_ids`时传递的可以表示的不同标记的数量
		  - * hidden\_​​size （`int`，_可选_，默认为 4096）— 隐藏表示的维度。
		  - * middle\_size（`int`，_可选_，默认为 22016）— MLP 表示的维度。
		  - * num\_hidden\_​​layers（`int`，_可选_，默认为 32）— Transformer 编码器中的隐藏层的数量。
		  - * num\_attention\_heads（`int`，_可选_，默认为 32）— Transformer 编码器中每个注意层的注意头的数量。
		  - * num\_key\_value\_heads（`int`，_可选_，默认为 32）— 这是应用于实现分组查询注意的 key\_value 头的数量。如果为`num_key_value_heads=num_attention_heads`，则模型将使用多头注意 (MHA)，如果为`num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32\`。
		  - * hidden\_​​act （`str`或`function`，_可选_，默认为`"silu"`）——解码器中的非线性激活函数（函数或字符串）。
		  - * max\_position\_embeddings（`int`，_可选_，默认为 32768）——此模型可能使用的最大序列长度。
		  - * initializer\_range（`float`，_可选_，默认为 0.02）— 用于初始化所有权重矩阵的 truncated\_normal\_initializer 的标准差。
		  - * rms\_norm\_eps （`float`，_可选_，默认为 1e-06）— rms 标准化层使用的 epsilon。
		  - * use\_cache ( `bool`，_可选_，默认为`True`) — 模型是否应返回最后的键/值注意（并非所有模型都使用）。仅在 时相关`config.is_decoder=True`。
		  - * tie\_word\_embeddings （`bool`，_可选_，默认为`False`） - 模型的输入和输出词嵌入是否应该绑定。
		  - * rope\_theta（`float`，_可选_，默认为 10000.0）— RoPE 嵌入的基准周期。
		  - * use\_sliding\_window（`bool`，_可选_，默认为`False`）—是否使用滑动窗口注意力。
		  - * slider\_window（`int`，_可选_，默认为 4096）— 滑动窗口注意 (SWA) 窗口大小。如果未指定，则默认为`4096`。
		  - * max\_window\_layers（`int`，_可选_，默认为 28）— 使用 SWA（滑动窗口注意力）的层数。底层使用 SWA，顶层使用完全注意力。
		  - * tention\_dropout （`float`，_可选_，默认为 0.0）— 注意概率的丢失率。
		  - ### Ollama — Qwen2
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">ollama serve
		  - <span style="color:#007400"># 你需要在使用 ollama 时保持此服务运行</span></span></span>
		  - ```
		  - 要提取模型检查点并运行模型，请使用该`ollama run`命令。您可以通过在 后添加后缀来指定模型大小`qwen2`，例如`:0.5b`、`:1.5b`、`:7b`或`:72b`：
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">ollama run qwen2:7b
		  - #要退出，请输入“/bye”并按 ENTER</span></span>
		  - ```
		  - 您还可以通过其与 OpenAI 兼容的 API 访问 ollama 服务。请注意，您需要 (1)`ollama serve`在使用该 API 时保持运行，以及 (2)`ollama run qwen2:7b`在使用此 API 之前执行以确保模型检查点已准备就绪。
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424"><span style="color:#aa0d91">从</span>openai<span style="color:#aa0d91">导入</span>OpenAI
		  - 客户端 = OpenAI(
		  - base_url= <span style="color:#c41a16">'http://localhost:11434/v1/'</span> ,
		  - api_key= <span style="color:#c41a16">'ollama'</span> ,   <span style="color:#007400"># 需要但被忽略</span>
		  - chat_completion = client.chat.completions.create(
		  - messages=[
		  - {
		  	- <span style="color:#c41a16">'role'</span> : <span style="color:#c41a16">'user'</span> ,
		  	- <span style="color:#c41a16">'content'</span> : <span style="color:#c41a16">'说这是一个测试'</span> ,
		  - }
		  - ],
		  - model= <span style="color:#c41a16">'qwen2:7b'</span> ,
		  - )</span></span>
		  - ```
		  - ### 使用 Alpaca 数据集对 Qwen 2 进行微调
		  - 以下代码使用 Alpaca 数据集对 Qwen2–0.5B 语言模型进行微调。语言模型对于文本生成、摘要和问答等自然语言处理任务非常有效。
		  - **先决条件：**所需的软件包和库包括 Unsloth、Xformers (Flash Attention)、trl、peft、accelerate、bitsandbytes、transformers 和 datasets。此代码旨在在 Google Colab 或兼容环境中运行。
		  - **步骤 1：**安装依赖项
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">!pip install <span style="color:#c41a16">"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"</span>
		  - !pip install --no <span style="color:#aa0d91">-deps</span> xformers <span style="color:#c41a16">"trl<0.9.0"</span> peft 加速 bitsandbytes</span></span>
		  - ```
		  - **步骤 2**：加载模型和标记器
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">从 unsloth<span style="color:#aa0d91">导入</span>FastLanguageModel
		  - <span style="color:#aa0d91">导入</span> <span style="color:#5c2699">torch </span>
		  - <span style="color:#3f6e74">max_seq_length </span> =  <span style="color:#1c00cf">2048</span>
		  - dtype = <span style="color:#5c2699">None </span>
		  - <span style="color:#3f6e74">load_in_4bit </span> =  <span style="color:#5c2699">True </span>
		  - <span style="color:#3f6e74">fourbit_models </span> = [
		  - <span style="color:#c41a16">"unsloth/Qwen2-0.5b-bnb-4bit"</span> ,
		  - model, tokenizer = FastLanguageModel.from_pretrained(
		  - model_name = <span style="color:#c41a16">"unsloth/Qwen2-0.5B"</span> ,
		  - max_seq_length = max_seq_length,
		  - dtype = dtype,
		  - load_in_4bit = load_in_4bit,
		  - )</span></span>
		  - ```
		  - FastLanguageModel.from\_pretrained() 函数加载 Qwen2–0.5B 模型及其 tokenizer。参数 max\_seq\_length、dtype 和 load\_in\_4bit 用于配置模型。
		  - **步骤 3：**应用 PEFT（参数有效微调）
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">模型 = FastLanguageModel.get_peft_model(
		  - 模型,
		  - r = <span style="color:#1c00cf">16</span> ,
		  - target_modules = [ <span style="color:#c41a16">“q_proj”</span> “ <span style="color:#c41a16">k_proj” </span><span style="color:#c41a16">“v_proj </span><span style="color:#c41a16">” “o_proj”</span> “
		  - <span style="color:#c41a16">gate_proj”</span> “ <span style="color:#c41a16">up_proj”</span> “ <span style="color:#c41a16">down_proj”</span> ,], lora_alpha
		  - = <span style="color:#1c00cf">16</span> ,
		  - lora_dropout = <span style="color:#1c00cf">0</span> ,
		  - 偏见 = <span style="color:#c41a16">“无”</span> ,
		  - use_gradient_checkpointing = “ <span style="color:#c41a16">unsloth”</span> ,
		  - random_state = <span style="color:#1c00cf">3407</span> ,
		  - use_rslora = <span style="color:#aa0d91">False</span> ,
		  - loftq_config = <span style="color:#aa0d91">None</span> ,
		  - )</span></span>
		  - ```
		  - FastLanguageModel.get\_peft\_model() 函数用于将 PEFT 应用于加载的模型。各种参数（例如 r、target\_modules、lora\_alpha、lora\_dropout、bias、use\_gradient\_checkpointing、random\_state、use\_rslora 和 loftq\_config）用于配置 PEFT 过程。
		  - **步骤 4：**定义羊驼提示
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424">alpaca_prompt = <span style="color:#c41a16">"""下面是描述任务的指令，与提供进一步上下文的输入配对。编写适当完成请求的响应。
		  - ### 指令：
		  - {}
		  - ### 输入：
		  - {}
		  - ### 响应：
		  - {}"""</span>
		  - EOS_TOKEN = tokenizer.eos_token <span style="color:#007400"># 必须添加 EOS_TOKEN </span>
		  - <span style="color:#aa0d91">def </span> formatting_prompts_func ( <span style="color:#5c2699">examples</span> ):
		  - instructions = examples[ <span style="color:#c41a16">"instruction"</span> ]
		  - input = examples[ <span style="color:#c41a16">"input"</span> ]
		  - output = examples[ <span style="color:#c41a16">"output"</span> ]
		  - texts = []
		  - <span style="color:#aa0d91">for</span> instructions, <span style="color:#5c2699">input</span> , output <span style="color:#aa0d91">in </span> <span style="color:#5c2699">zip</span> (instructions, input, output):
		  - text = alpaca_prompt.format (instruction, <span style="color:#5c2699">input</span> , output) + EOS_TOKEN         texts.append(text) <span style="color:#aa0d91">return</span> { <span style="color:#c41a16">"text"</span> : texts, <span style="color:#5c2699">}</span>
		  - </span></span>
		  - ```
		  - alpaca\_prompt 变量是格式化输入数据的模板。EOS\_TOKEN 和 formatting\_prompts\_func() 函数用于准备训练数据。
		  - **步骤5：**加载并预处理数据集
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424"><span style="color:#aa0d91">从</span>数据集<span style="color:#aa0d91">导入</span>load_dataset
		  - 数据集 = load_dataset( <span style="color:#c41a16">"yahma/alpaca-cleaned"</span> , split = <span style="color:#c41a16">"train"</span> )
		  - 数据集 = 数据集.map <span style="color:#5c2699">(</span> formatting_prompts_func, batched = <span style="color:#aa0d91">True</span> ,)</span></span>
		  - ```
		  - 使用 Hugging Face 数据集库中的 load\_dataset() 函数加载 Alpaca 数据集。使用 map() 函数将 formatting\_prompts\_func() 函数应用于数据集。
		  - **步骤 6：**设置训练配置
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424"><span style="color:#aa0d91">从</span>trl<span style="color:#aa0d91">导入</span>SFTTrainer
		  - <span style="color:#aa0d91">从</span>transformers<span style="color:#aa0d91">导入</span>TrainingArguments
		  - <span style="color:#aa0d91">从</span>unsloth<span style="color:#aa0d91">导入</span>is_bfloat16_supported
		  - trainer = SFTTrainer(
		  - model = model,
		  - tokenizer = tokenizer,
		  - train_dataset = dataset,
		  - dataset_text_field = <span style="color:#c41a16">"text"</span> ,
		  - max_seq_length = max_seq_length,
		  - dataset_num_proc = <span style="color:#1c00cf">2</span> ,
		  - args = TrainingArguments(
		  - per_device_train_batch_size = <span style="color:#1c00cf">2</span> ,
		  - gradient_accumulation_steps = <span style="color:#1c00cf">8</span> ,
		  - <span style="color:#007400"># 使用 num_train_epochs = 1, warmup_ratio 进行完整训练运行！</span>
		  	- warmup_steps = <span style="color:#1c00cf">20</span> ,
		  - max_steps = <span style="color:#1c00cf">120</span> ,
		  - learning_rate = <span style="color:#1c00cf">5e-5</span> ,
		  - fp16 = <span style="color:#aa0d91">not</span> is_bfloat16_supported(),
		  - bf16 = is_bfloat16_supported(),
		  - logstash_steps = <span style="color:#1c00cf">1</span> ,
		  - optim = <span style="color:#c41a16">"adamw_8bit"</span> ,
		  - weight_decay = <span style="color:#1c00cf">0.01</span>，
		  - lr_scheduler_type = <span style="color:#c41a16">“线性”</span>，
		  - 种子= <span style="color:#1c00cf">3407</span>，
		  - output_dir = <span style="color:#c41a16">“输出”</span>，
		  - ），
		  - ）</span></span>
		  - ```
		  - trl 库中的 SFTTrainer 类用于训练语言模型。TrainingArguments 用于配置各种参数，例如批处理大小、梯度累积步骤、预热步骤、最大步骤、学习率、混合精度设置、记录步骤、优化器、权重衰减、学习率调度程序和种子。
		  - **步骤 7：**训练模型
		  - ```ba
		  - <span style="background-color:#f9f9f9"><span style="color:#242424"><span style="color:#836c28">训练师统计</span>= 训练师.train()</span></span>
		  - ```
		  - 调用 trainer.train() 函数启动训练过程。trainer\_stats 变量存储训练统计数据。
		  - 经过微调的模型可用于各种自然语言处理任务，例如文本生成、摘要和问答。
		  -
		  - ### 高亮
		  collapsed:: true
		  - > Hugging Face 的 Open LLM Leaderboard [⤴️](https://omnivore.app/me/qwen-2-qwen-2-qwen-2-csdn-1917d392cab#dff3c760-6d49-4b22-a724-ecc3fa516f1f)
		  - [Omnivore 入门 --- Getting Started with Omnivore](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2)
		  collapsed:: true
		  site:: [Omnivore](https://omnivore.app/zhangxinxin0216/getting-started-with-omnivore)
		  author:: unknown
		  labels:: [[插件类]]
		  date-saved:: [[Aug 23rd, 2024]]
		  - ### 高亮
		  collapsed:: true
		  - > * Saving 储蓄
		   * Reading 读数
		   * Organizing 组织
		   * Integrations 集成 [⤴️](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2#ce4b3b05-d311-4b9e-bdcc-462d6d7496e6)
		  - > **图书馆**是您 Omnivore 体验的中心 [⤴️](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2#843cb26a-b673-4a42-afe2-7341c64a2343)
		  - > 从 Mac 存储 PDF [⤴️](https://omnivore.app/me/omnivore-getting-started-with-omnivore-1917cecd3e2#81e8fe5b-a27c-4e36-a2cb-7900a36c1731)
		  - [The Secret Power of ‘Read It Later’ Apps（《哈佛商业评论》最近发表的一篇文章为这种新的神经现象命名：注意力缺陷特质）](https://omnivore.app/me/the-secret-power-of-read-it-later-apps)
		  site:: [Forte Labs](https://fortelabs.co/blog/the-secret-power-of-read-it-later-apps)
		  author:: Tiago Forte
		  labels:: [[插件类]]
		  date-saved:: [[Aug 21st, 2024]]
		  date-published:: [[Jan 24th, 2022]]
		  -
		  - https://omnivore.app/me/rag-csdn-1917dcec1e0
		  -
		  - https://omnivore.app/me/qwen-2-qwen-2-qwen-2-csdn-1917d392cab
		  - [探索知识管理新维度：Logseq Anki Sync-CSDN博客](https://blog.csdn.net/gitblog_00070/article/details/139165044?ops_request_misc=&request_id=&biz_id=102&utm_term=logseq%E6%8F%92%E4%BB%B6&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-139165044.142^v100^pc_search_result_base3&spm=1018.2226.3001.4187)
		  collapsed:: true
		  - [logseq-anki-syncAn logseq to anki syncing plugin with superpowers - image occlusion, card direction, incremental cards, and a lot more.项目地址:https://gitcode.com/gh\_mirrors/lo/logseq-anki-sync](https://gitcode.com/gh%5Fmirrors/lo/logseq-anki-sync/?utm%5Fsource=artical%5Fgitcode&index=top&type=card&webUrl)
		  - 在信息爆炸的时代，高效的知识管理和记忆工具显得尤为重要。Logseq Anki Sync 是一款强大的插件，它将流行的思维导图软件 Logseq 与著名的间隔重复学习应用 Anki 直接连接起来，让您可以在两个平台上无缝同步和强化学习材料。
		  - ### 项目简介
		  - Logseq Anki Sync 是一个创新的插件，允许您将 Logseq 中的笔记内容以丰富多样的形式（包括 Markdown 和 Org 模式）导入到 Anki 中进行复习。这个插件不仅支持常规的文本同步，还涵盖了 Logseq 的许多高级特性，如 PDF 注释、数学公式等。通过利用 Anki 的智能算法和个性化设置，您可以打造一个个性化的学习系统。
		  - ### 技术分析
		  - 该插件运用了先进的哈希技术来检测 Logseq 或 Anki 中的变化，确保只同步更新的内容，节省时间。此外，它支持多种卡片类型，如图像遮挡、PDF 遮挡、双向卡片等，并且完美地在 Anki 中呈现 Logseq 的Markdown和Org模式内容。安装过程简单明了，只需几步即可完成。
		  - ### 应用场景
		  - Logseq Anki Sync 尤其适用于：
		  - * 知识工作者：需要对大量信息进行整理和记忆的人士。
		  - * 学生：在备考时，可以创建特定的复习集合，按需学习。
		  - * 教师：为课程创建动态的学习资源库。
		  - ### 项目特点
		  - 1. **全面的渲染支持**：不仅同步文本，还同步块引用、页面引用、PDF注解、数学公式等各种复杂内容。
		  - 2. **丰富的附加功能**：支持图像遮挡、PDF处理、方向卡、增量卡片等，提升学习体验。
		  - 3. **充分利用Anki功能**：与Anki的高级特性兼容，如过滤牌组、热力图、语音合成等。
		  - 4. **高速同步**：采用最新哈希技术，仅同步变更，提高效率。
		  - ### 结语
		  - 借助 Logseq Anki Sync，您可以构建一个既强大又灵活的知识管理系统，将分散的思考笔记转化为富有成效的记忆练习。立即尝试并发掘它的潜力，让您的知识管理更上一层楼！如果你喜欢这个工具，请考虑赞助支持开发者，让他们能够继续优化和扩展这个项目。
		  - [logseq-anki-syncAn logseq to anki syncing plugin with superpowers - image occlusion, card direction, incremental cards, and a lot more.项目地址:https://gitcode.com/gh\_mirrors/lo/logseq-anki-sync](https://gitcode.com/gh%5Fmirrors/lo/logseq-anki-sync/?utm%5Fsource=artical%5Fgitcode&index=bottom&type=card&webUrl)
		  - [推荐一款神奇的Logseq插件：Luckysheet增强版日志管理体验-CSDN博客](https://blog.csdn.net/gitblog_00065/article/details/137366090?ops_request_misc=&request_id=&biz_id=102&utm_term=logseq%E6%8F%92%E4%BB%B6&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-137366090.142^v100^pc_search_result_base3&spm=1018.2226.3001.4187)
		  - [三款顶级开源RAG (检索增强生成)工具：Verba、Unstructured 和 Neum-CSDN博客](https://blog.csdn.net/XianxinMao/article/details/136575056?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-136575056-blog-141191700.235%5Ev43%5Epc_blog_bottom_relevance_base9&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-136575056-blog-141191700.235%5Ev43%5Epc_blog_bottom_relevance_base9&utm_relevant_index=1)
		  collapsed:: true
		  - **概述**
		  - 随着企业对话式数据处理需求的提升，面临的挑战是数据隐私性和缺乏企业级解决方案。虽然类似LangChain能在短时间内构建RAG应用，但忽视了文档解析、多来源数据ETL、批量处理、访问控制等问题。此文评估了三款开源RAG工具的潜力，专为生产环境设计。
		  - #### Verba：理想的RAG选择
		  - * Weaviate提供的Verba项目强调易用性，是RAG应用而非框架。
		  - * 提供与多种嵌入模块的集成，如GPT3/4和Cohere。
		  - * 支持PDF和纯文本等多种文件格式的解析。
		  - * 提供快速入门和Docker部署选项。
		  - #### Unstructured：专注数据ETL的RAG框架
		  - * 专注于统一和转换不同数据格式以适配向量数据库和LLM框架。
		  - * 提供多种文件类型支持和20多种数据源。
		  - * 基于文档模型的元素转换和优化。
		  - **快速开始指南：**
		  - * 使用pip安装客户端。
		  - * 注册API密钥或使用Docker自主托管API。
		  - #### Neum：管道式的RAG框架
		  - * Meruem新推出的RAG平台强调源、连接器和终点等清晰定义。
		  - * 关注大规模数据摄取问题，支持语义分块（LLM定义的分块策略）。
		  - * 提供无代码管线构建器，并有清晰语法的Pipeline配置。
		  - **快速开始指南：**
		  - * 安装neumai后，按照指南创建第一个Pipeline。
		  - * 预期未来会增加Docker支持和更完善的文档。
		  - #### 展望
		  - * Verba、Unstructured和Neum可结合使用，构建接近完整的生产就绪型RAG应用。
		  - * 相关工具的交叉整合将驱动开源RAG工具的持续发展。
		  - #### 小结
		  - 开源社区正在积极推进RAG工具的发展，提供企业级的数据处理解决方案。不论是Verba提供的用户界面，还是Unstructured和Neum的文档处理与管线代码，都预示了RAG技术的光明未来。
		  - [姜萍的奥数赛成绩即将公布，网友等不及了：8月份了？ (baidu.com)](https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_8960619479621035962%22%7D&n_type=-1&p_from=-1)
		  collapsed:: true
		  - #深度好文计划#时间过得真快。一转眼，从6月又来到了8月。姜萍的数赛排名，想必也要出来了吧。
		  - 6份，阿里巴巴全球数赛后，一名来自江苏涟水的中专生，姜萍获得了全球排名第12名。一时间，因为这位自带话题的女孩，从此出圈了。
		  - 排在她前面的，都是来自全球各大名校，都是大有来头，都是有很强硬的数学背景的人才。唯独她，姜萍，一名来自中专的17岁女孩。
		  - 她不仅不是大学生，也不是来自特别富裕的家庭，但正因为这巨大的差别，她参加的阿里巴巴的数赛排名12，让她有了不一样的光芒。
		  - ![](https://pics1.baidu.com/feed/c83d70cf3bc79f3d9a2f72478d5a481f708b2980.jpeg@f_auto?token=0fa10631829d5a3f66f74830386e74f9)
		  - 对于这样的一位天才少女，人民日报也是亲自发文报道。想一想，在平凡的人中间，难有几个人可以凭借着平凡的身世，凭借着一次竞赛，可以突破圈层，可以达到令全世界注目的荣耀，唯有用天才，才能解释得了。
		  - 因为姜萍的出圈，她的父母虽然年龄很大，却因为要抚育两个孩子，依然在外地打工，而她的母亲因为身体不好，治疗后，更是欠下了一笔债。
		  - 而她的姐姐，是一名在读大学生……等等这些信息，因为网友们的疯狂追慕，从而从侧面了解了姜萍一家的情况。正是因为这个来自平凡，来自中专的女孩，却获得了如此的排名，一夜成名，也是在常理之中了。
		  - 而她的老师，王闰秋，作为姜萍的班主任兼数学老师，也被媒体采访。被网友们欣喜地称之为“千里马常有，而伯乐不常有。”里伯乐的伯乐。
		  - ![](https://pics1.baidu.com/feed/f3d3572c11dfa9ecbb4e5f6e552b720d908fc174.jpeg@f_auto?token=247bcf550098fc60ef8d2f667071b83a)
		  - 对于姜萍能获得这样一份荣誉，尤其是出身与家世这样巨大的落差，不少同样平凡的人为她高兴，为她庆祝，但有些身份不一样的人却在质疑她，甚至公开诽谤她。
		  - 那些质疑的人，从她的板书开始，截图详细地道来，似乎有理有据。一时间，有关姜萍的各种质疑和议论，如波涛般纷至杳来。质疑她一位毫无数学经验的女孩，如何能在这全球性的竞赛中以排名12胜出，其中肯定藏有不可告人的弄虚作假，或者背后有团队在动作。
		  - 甚至还有一位叫赵斌的北大硕士实名质疑，并愿意用500万对赌。结果虽然不了了之，但因为实名跳出来，倒也吸引了一大波的话题 和争议。
		  - 还有北大数学家袁新意等人，甚至还出现了多名参赛选手发联名信，要求达摩院公开姜萍初赛试卷的事情。
		  - ![](https://pics5.baidu.com/feed/f11f3a292df5e0fea6ad3c6d089bb1a65fdf722a.jpeg@f_auto?token=13f7b262433d9c4be63cb4f585d5105c)
		  - 这些质疑还不是简单的一句话，他们从多个维度给出了自己的依据，尤其是数学家袁新意，从非常理性的角度表达了自己的观点，希望姜萍能自证清白。
		  - 后面又出现了37名学生联名请愿，要求姜萍当众考试。这样的一番操作，让希望姜萍好，愿意姜萍好的平头老百姓无语了，也愤怒了。一个女孩能这样有出息，他们居然不高兴，反而吹毛求疵质疑这质疑那，也让人悲哀。
		  - 面对这些人的质疑，姜萍及家属并没有回应。姜萍依然在规定的日子参加了决赛。
		  - 于是，所有人都把希望寄托在决赛的排名上，希望姜萍再次以实力来打这一众无聊的有身份有地位人的嘴。
		  - 依据2024阿里全球数赛组织方达摩院消息，8月份将公布决赛成绩，现在已经是8月份了，很多网友已经等不及了，纷纷在达摩院的视频下方打卡留言，提醒尽快公布姜萍的竞赛成绩。
		  - ![](https://pics4.baidu.com/feed/18d8bc3eb13533fa889d6a41fc28781140345bbb.jpeg@f_auto?token=11ca71e8ba559f568044cc2749810f7f)
		  - 有趣的是，进入8月后，几乎每天都有大量网友在宣传姜萍的视频下方留言，纷纷提醒时间到了。
		  - 不过，达摩院并未回复任何网友的留言，这让翘首以待的网友感到特别的心焦。
		  - 更让网友心焦的是，达摩院自6月份之后，就再没有继续更新任何视频。据说，此前几乎是不断更的。网友们越是想早一点知道竞赛的结果，对方越是一波无漾。
		  - 你说，这是不是一拳打在了棉花上。任是你着急，也是没有任何作用的。
		  - ![](https://pics0.baidu.com/feed/21a4462309f790522cacbed4340852c479cbd5b7.jpeg@f_auto?token=9fede491df283ac24697fd6dafd9e37f)
		  - 于是，大家只好静下心来等待。希望到了时间，达摩陆军可以如期公布决赛名单。
		  - 大家如此期待决赛名单的宣布，我想，并不是去关注第一名，第二名，甚至前十名，而是在关注此前出现争议，作为一名学习服装设计的中专生的姜萍。
		  - ![](https://pics2.baidu.com/feed/8694a4c27d1ed21b130fe18c9a9558ca53da3f9a.jpeg@f_auto?token=a4d98068b7c27aceff3e5976770c156b)
		  - 在这种情况下，喜欢姜萍的粉丝希望数学大赛主办方尽快公布决赛成绩，帮助姜萍澄清事实，要用实力来证明自己的清白；当然也有部分网友，希望用结果来证实自己的质疑。
		  - [姜萍决赛成绩未公布，家属否认拿第8名，这场闹剧该结束了 (msn.cn)](https://www.msn.cn/zh-cn/news/other/%E5%A7%9C%E8%90%8D%E5%86%B3%E8%B5%9B%E6%88%90%E7%BB%A9%E6%9C%AA%E5%85%AC%E5%B8%83-%E5%AE%B6%E5%B1%9E%E5%90%A6%E8%AE%A4%E6%8B%BF%E7%AC%AC8%E5%90%8D-%E8%BF%99%E5%9C%BA%E9%97%B9%E5%89%A7%E8%AF%A5%E7%BB%93%E6%9D%9F%E4%BA%86/ar-AA1ppXrG?ocid=msedgntp&pc=U531&cvid=66cbedba42cd426989a1aca51648733a&ei=13)
		  - [开源RAG框架汇总-CSDN博客](https://blog.csdn.net/qq_33137873/article/details/138683590?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-138683590-blog-141191700.235%5Ev43%5Epc_blog_bottom_relevance_base9&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-138683590-blog-141191700.235%5Ev43%5Epc_blog_bottom_relevance_base9&utm_relevant_index=2)
		  collapsed:: true
		  - 本文搜集了一些开源的基于LLM的RAG（Retrieval-Augmented Generation）框架，旨在吸纳业界最新的RAG应用方法与思路。如有错误或者意见可以提出，同时也欢迎大家把自己常用而这里未列出的框架贡献出来，感谢\~
		  - ## RAG应用框架
		  - * 项目地址：https://github.com/infiniflow/ragflow
		  - * 简介：RAGFlow 是一款基于深度文档理解构建的开源 RAG（Retrieval-Augmented Generation）引擎。RAGFlow 可以为各种规模的企业及个人提供一套精简的 RAG 工作流程，结合大语言模型（LLM）针对用户各类不同的复杂格式数据提供可靠的问答以及有理有据的引用。
		  - * 特性：OCR、**内置多种文档切分模板**、文档切分可视化并且可修改、兼容多种文档数据类型
		  - * 架构：
		  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/c34b905634e04be0adfd2450be41e4cb.png#pic_center)
		  - * 硬件要求：CPU >= 4 核、RAM >= 16 GB、Disk >= 50 GB、Docker >= 24.0.0 & Docker Compose >= v2.26.1
		  - * 项目地址： https://github.com/netease-youdao/QAnything
		  - * 简介：QAnything ( Q uestion based on Anything ) 是贡献支持任何格式文件或数据库的本地知识库问答系统，可断网安装使用。您的任何格式的本地文件都可以往里扔，即可获得准确、快速、靠谱的问答体验。
		  - * 特性：支持离线安装使用、**跨语种问答**、**粗排和精排的二阶段召回**
		  - * 架构：
		  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/d34724e9c483434dbf88896885b839d8.png#pic_center)
		  - * 硬件要求：最低CPU即可；使用GPU环境需要NVIDIA GPU Memory >= 4GB (use OpenAI API) & Docker Desktop >= 4.26.1（131620）
		  - * 项目地址：https://github.com/open-webui/open-webui
		  - * 简介：Open WebUI 是一个可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线操作。它支持各种 LLM 运行程序，包括 Ollama 和 OpenAI 兼容的 API。
		  - * 特性：**原生支持Ollama**、**支持安装和卸载模型**、**支持多模态模型**、**支持切换模型**、**多用户管理**
		  - * 架构：
		  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/413e61ac41fb4c1d981194df467de82f.gif#pic_center)
		  - * 硬件要求：最低CPU即可，使用GPU环境需要NVIDIA GPU Memory >= 4GB (取决于使用Ollama的模型大小)
		  - * 项目地址：https://github.com/labring/FastGPT
		  - * 简介：FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景！
		  - * 特性：**支持应用编排**、**免登录分享**、**支持接入飞书、企业微信等应用**
		  - * 架构：
		  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/4a0319a9d1834a3aae3b219557bb9128.webp#pic_center)
		  - * 硬件要求：CPU >= 2 核、RAM >= 4 GB用于安装数据库，GPU取决于使用的模型
		  - * 项目地址：https://github.com/chatchat-space/Langchain-Chatchat
		  - * 简介：基于 ChatGLM 等大语言模型与 Langchain 等应用框架实现，开源、可离线部署的检索增强生成(RAG)大模型知识库项目。
		  - * 特性：算是比较早期的RAG框架了，使用的基本全是python的框架。该项目是一个可以实现**完全本地化**推理的知识库增强方案, 重点解决数据安全保护，私域化部署的企业痛点。支持市面上主流的本地大语言模型和Embedding模型，支持开源的本地向量数据库。 本开源方案采用Apache License，可以**免费商用，无需付费**。
		  - * 架构：
		  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/d891ea4ac20e4827bea7845a34de0990.png#pic_center)
		  - * 硬件要求：对GPU要求较高
		  - * 项目地址：https://github.com/1Panel-dev/MaxKB
		  - * 简介：MaxKB 是一款基于 LLM 大语言模型的知识库问答系统。MaxKB = Max Knowledge Base，旨在成为企业的最强大脑。
		  - * 特性：开箱即用，支持直接上传文档、自动爬取在线文档；支持零编码快速嵌入到第三方业务系统；支持对接主流的大模型，包括 Ollama 本地私有大模型以及API调用
		  - * 架构： 前端：Vue.js 后端：Python / Django LangChain：LangChain 向量数据库：PostgreSQL / pgvector
		  - 大模型：Azure OpenAI、OpenAI、百度千帆大模型、Ollama、通义千问、Kimi、智谱 AI、讯飞星火
		  - * 硬件要求：
		  - 操作系统：Ubuntu 22.04 / CentOS 7 64 位系统 CPU/内存： 推荐 2C/4GB 以上
		  - 磁盘空间：100GB
		  -
		  -
		  - ## [[阅读清单]]
		  - 封面:: 1
		  书名:: 战争与和平
		  作者:: 俄国人
		  状态:: 待读
		  开始时间::  [[Aug 25th, 2024]] 
		  结束时间::  [[Aug 29th, 2024]] 
		  评分:: 5
		  类型:: 战争
		  年份:: 1960
		  -
		  -
		  - ## [[阅读清单]]
		  - 封面:: 2
		  书名:: 枪炮 战争与细菌
		  作者:: 美国人
		  状态:: 未读
		  开始时间::  [[Aug 23rd, 2024]] 
		  结束时间::  [[Aug 25th, 2024]] 
		  评分:: 7
		  类型:: 政论
		  年份:: 1950
		  -
		  -
		  - {{query (and [[阅读清单]] (property :书名) (property :作者) (property :封面))}}
		  query-table:: true
		  -
		  - /ca
		  - ```calc
		  1+1
		  1/2
		  1/3
		  2*8
		  3^3
		  sin(45)
		  
		  ```
			- > Hugging Face 的 Open LLM Leaderboard [⤴️](https://omnivore.app/me/qwen-2-qwen-2-qwen-2-csdn-1917d392cab#dff3c760-6d49-4b22-a724-ecc3fa516f1f)
-
-
-
-
-
-
-
-
-
-