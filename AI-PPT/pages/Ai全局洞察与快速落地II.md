- ---
- Ai全局洞察与快速落地II
-
- # Embedding** ****
  id:: 66a8ee81-ec61-4f90-a6e7-66c375018fb8
	- ## ** ** **向量化/** **Embedding** ** 概念**
	  id:: 66a731ed-3184-4336-8d71-07adea75f7da
		- ### **Embedding概念**
			- Embedding是一种将离散数据（如单词、类别标签等）转换为连续向量表示的技术。在自然语言处理中，Embedding通过将每个单词映射到一个高维空间中的点（即向量），使得具有相似含义的单词在向量空间中彼此接近。这样，模型就可以在向量空间中进行数学运算，如计算向量之间的距离或相似度，从而理解单词之间的关系。
			  id:: 66a9e422-03ce-4233-9a3e-3aec5d3d04b3
			-
		- **Embedding特点**
			- 语义信息保留：Embedding向量能够捕捉单词的语义信息，使得模型能够理解单词之间的复杂关系。
			- 灵活性：可以根据不同的任务和数据集学习得到不同的Embedding表示。
			- ---
		- **Embedding技术**
		  id:: 66bc5bc2-d231-423f-aa61-8e9648a237ea
			- Embedding技术就像是这个字典的现代版，但它不是简单地给每个单词一个编号，而是给每个单词一个复杂的数字“指纹”。 这个“指纹”是一个**由很多数字组成的向量**，就像是一串数字序列。这个序列能够捕捉到单词的很多特性:
			- 比如它的意思、它在句子中的作用，甚至是它的情感色彩。如果我们有“快乐”和“悲伤”这两个词，Embedding技术会生成两个不同的向量。尽管这两个词在字典里可能紧挨着，但它们的向量会相差很远，因为它们表达的情感是相反的，计算机可以通过比较这两个向量的距离，来理解这两个词在情感上的不同。
			- ---
		- **Embedding层**
			- Embedding 层是神经网络中的一个基础组件，它将整数索引（代表离散的类别）映射到固定大小的密集向量。每个整数索引对应一个唯一的向量，这些向量通常在训练过程中学习得到。在 NLP 中，Embedding 层通常用于将单词（通过其索引表示）映射到向量，这些向量可以**捕捉到单词的语义信息。**
			- 在机器学习和深度学习中，嵌入层（embedding layer）是一种特殊的神经网络层，它的作用是将离散的输入数据（如单词、物品、用户ID等）映射到连续的向量表示。
			- 这种映射是通过一个可训练的权重矩阵实现的，该矩阵的行数等于输入数据的类别数（例如，词汇表中的单词数），列数等于嵌入向量的维度。
			-
			- ---
		- **Embedding示例**
			- 例如，假设我们有一个包含 10,000 个单词的词汇表，我们想要将每个单词映射到一个 128 维的向量。那么，Embedding 层将会有 10,000 个输入（每个单词一个）和 128 个输出（每个单词的向量表示）。
			- 在 PyTorch 中，Embedding 层的基本用法如下：
			  import torch.nn as nn
			  假设词汇表大小为 10,000，嵌入向量的维度为 128
			  embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=128)
			  embedding的输入时是整数张量，每个整数代表一个词汇的索引，输出是一个浮点数张量，每个点数都代表着对应词汇的词嵌入向量。
			  tips：0阶张量表示的是一个标量，1阶张量表示的是一个向量，2阶张量表示的是一个矩阵
			-
		- **向量化**
			- 在深度学习中，向量化通常指的是将输入数据（如文本、图像等）转换为固定大小的向量表示，这些向量可以进一步用于模型的训练和推理。
			- **特点：**
			- **· **高效性：****通过同时对多个数据元素执行相同的操作，显著提高计算效率。**
			- **· **简化数据处理：****将复杂的数据结构简化为易于处理的向量形式。**
	- ___
	- ** ** **向量化/** **Embedding**** 图示 **
		- 如下图所示，文本向量化模型通过将“生活日常知识”转换为数值向量，可以将文本信息表示成能够表达文本语义的向量
	- ___
	- ** ** **Embedding 层的****作用和语义理解**
		- **一）嵌入层的作用**
			- 1. 降维：**高维到低维的降维实现过程及原理**
				- 在自然语言处理和机器学习中，**高维到低维的降维实现过程**：
					- **高维空间****通常指的是包含****大量特征或属性的数据集****所在的空间。这些特征或属性可以是数值型的，也可以是分类型的，它们共同描述了数据点的各个方面。**
					- **低维空间****则是指维度较低的空间，即****包含较少特征或属性的数据集所在的空间****。在低维空间中，数据点可以通过较少的参数来描述，这通常使得数据的可视化和分析变得更加容易。**
					- **降维的过程****就是将数据点从高维空间转换到低维空间的过程，同时尽可能保持****数据点****之间的****相对距离和结构****不变。（即：尽可能保留数据的关键信息）**
				- 在自然语言处理中，**降维通常通过嵌入层**来实现：
					- 如Word2Vec、GloVe或Transformer模型中的嵌入层，将高维的one-hot编码转换为低维的连续向量，这些向量能够捕获单词间的语义和语法关系，从而在低维空间中表示文本数据。
					- **降维的目的****是在****减少数据维度的同时，尽可能保留数据的关键特征****。**
					- **最终目的**是将高维的文本向量压缩到低维空间中，是为了能够在低维空间中更有效地分析和处理数据，以便于分析和理解。例如通过可视化来发现模式、通过简化模型来减少计算成本、或通过压缩数据来减少存储需求。
					- 嵌入层中的降维确实可以被视为一种在保持语义或关键特征前提下的数据压缩技术。不仅减少了数据的维度，而且还保留了数据的重要结构和信息
						- **保持语义：**嵌入层通过将高维的one-hot编码转换为低维的连续向量，单词的向量表示仍然能够反映出它们在语义上的相似性或差异
						- **关键特征保留：**在降维过程中，嵌入层通过学习，能够保留和浓缩数据中的关键特征。例如，一个单词的嵌入向量可以捕获到其在不同上下文中的使用频率、语法角色和语义含义，这些都是理解文本数据的关键特征。
						- **数据压缩**：从信息论的角度来看，嵌入层的降维过程可以视为一种数据压缩技术。通过将**高维的离散**表示转换为**低维的连续**表示，不仅减少了数据存储和处理所需的资源，还通过学习得到了更加紧凑和高效的数据表示。
						- **效率与性能**：这种压缩技术在提高计算效率和模型性能方面发挥了重要作用。低维的嵌入向量使得模型在进行计算时更加高效，同时也能够捕获到数据的深层结构，从而在各种NLP任务中表现出色。
						  id:: 66bc0756-7037-4796-9d5d-7c9a649d3d3f
			- **2.  泛化：**嵌入层可以学习到输入数据的分布式表示，这种表示能够捕捉到数据中的语义或结构信息，有助于模型泛化到未见过的数据。
			- 3. **相似性：**在嵌入空间中，语义上相似或相关的输入数据（如单词的同义词、物品的类似类别等）会被映射到相近的向量，这使得模型能够利用这些相似性。
			- 4. **效率：**相比于使用one-hot编码，嵌入向量提供了更加紧凑的数据表示，可以减少模型的参数数量，提高训练和推理的效率。
		- **二）嵌入层被定义为网络的第一个隐藏层；99%的人未解的“参数数量”解释**
			- **Embedding Dimensions（嵌入尺寸）**
				- 所谓的“低纬”指的是：Embedding Dimensions（嵌入尺寸）；指在一个嵌入模型中，用于表示每一个独特输入（如单词、句子或实体）的**向量维度**数。换句话说，它定义了每个嵌入向量在向量空间中的长度或宽度，即有多少个数值组成这个向量；
				- 增加嵌入维度能够**为模型提供更丰富的表示能力，允许模型学习到****更复杂的特征和关系****，但也相应地****增加了模型的复杂性和计算成本****。因此，选择合适的嵌入维度是设计嵌入模型时的一个重要决策点；
				- **语义表示：****更高纬的Embedding Dimensions****能够为输入提供****更丰富、更细致的语义****表示。这意味着模型能够学习到更复杂的上下文关系和特征。
			- **Model Size**（模型大小）
				- **向量模型大小（参数数量）"Model Size (Million Parameters) 通常指的是在向量空间中表示模型所需的参数总量。 **
				  即：**Model Size=词汇表大小（num_embeddings）✖每个嵌入向量的维度（embedding_dim）**
				- num_embeddings 表示词汇表的大小，即模型能够区分的不同类别或单词的数量。
				- embedding_dim 是每个单词或类别对应的嵌入向量的维度，越多使模型越能捕获更复杂的语义特征
		- 三）本章小结
			- **嵌入是一种将对象映射到连续向量空间的技术。**
			  
			  在自然语言处理中，嵌入通常用于将单词或短语映射到高维向量空间中，以便计算它们之间的语义相似度。嵌入技术也被广泛应用于推荐系统、图像处理和其他机器学习任务中。通过学习嵌入表示，我们可以将复杂的结构化数据转换为连续的向量，从而方便进行进一步的分析和建模。
				-
-
-
-
-
-
-