## 用户痛点分析: 云栖大会中衍射/引导产生的痛点关键词
id:: 66ecbe97-461a-4b94-bbdd-9408b45712b0
	- 电能秒变智能：只因为读懂了语言！人类历史上最伟大的范式转变！
	- 没有一个可架构的平台，被聊天机器人这个形式给带到坑里去了；以至于大量的“数字资产”无处存放
	  collapsed:: true
		- 当前世界上最先进的笔记体系；人工智能离不开最好的背景墙/架构平台，一如互联网离不开实业一样；
		  collapsed:: true
			- Copilot依附的平台
				- 程序员们VS Code；IDE；
			- 我们：世界上当前最先进的笔记体系logseq：（官方首页介绍）
			  **[[#red]]==A safe space== for ** your thoughts. 
			  **[[#red]]==一个安全的空间==**，让你的思想自由
			  Designed to store your  **interests, questions, ideas, favorite quotes, reminders, reading and meeting notes ** easily and future-proof: 
			  旨在轻松存储您的**兴趣、问题、想法、最喜欢的名言、提醒、阅读和会议记录**，并且不会过时：
				- Open source  [[#green]]==开源==
				- Privacy first [[#green]]== 隐私第一==
				- Mobile apps  移动应用
				- Markdown files  Markdown 文件
				- Strong community  强大的社区
				- Localization  地方化
				- 150+ Plugins  150+ 插件
				- 30+ Themes  30+ 主题
			- [[#red]]==A new canvas  for your thoughts==. 您的[[#red]]==思想新画布==
			  Place any of your thoughts from the knowledge baseor new ones next to each other on an infinite canvas  to connect, associate and understand in new ways. 
			  将知识库中的任何想法或新想法彼此相邻放在无限的画布上，以新的方式联系、关联和理解。
			- New ideas will pop up with time. 
			  随着时间的流逝，新的想法会不断涌现。
			  Using Logseq helps you [[#red]]==organize your thoughts and ideas== 
			  使用 Logseq 可以帮助您[[#red]]==组织您的想法和想法==
			  so that you can  come up with new outputs more easily. 
			  这样您就可以更轻松地提出新的输出。
			- Overwhelmed and constantly afraid of losing your thoughts?
			  不知所措，不断害怕失去你的思想？
			  Everyday you’re bombarded with information.
			  每天你都会被信息轰炸。
			  Logseq helps you  turn this daily mess into [[#red]]==structured information.== 
			  Logseq 帮助您将日常混乱转化为[[#red]]==结构化信息==。
			  **Gain clarity** in your everyday life:
			  **在日常生活中获得清晰度**：
			- # **Gain clarity**  in your everyday life: 
			  **在日常生活中获得清晰度**：
		-
	- 副驾驶/超级助理：基于OpenAI大模型的logseq（双链笔记）的智能插件
	  collapsed:: true
		- 开发人工智能的程序员在如何使用？
		  collapsed:: true
			- 通义灵码：基于代码大模型的IDE智能插件
			- bing---->copilot
			- google浏览器右侧
		- 超级好副驾的标准：
			- 毫秒级的生成速度；恰到好处的生成时机；恰如其分的生成长度；
	- 心流被打断
	  collapsed:: true
		- 打开浏览页或对话框，很快半小时过去了；
		- 因为有幻觉，又因为有跳转，大模型的能力不等同于产品体验
	- 长期是天使，偶尔是魔鬼（幻觉带来的胡说八道）；** “[[_green]]==Lost in the middle==” /“迷失在中间”
		- 幻觉：即不符合事实的输出
	- 最后一公里/AI技术选型/可观测性/灵活性/容错性高、结构化强（Spring AI Alibaba 的系列启发）
	  collapsed:: true
		- 我应该使用 ChatGPT 还是 API？
			- [ChatGPT](https://chatgpt.com/) 为我们的模型提供了一个聊天界面和一系列内置功能，例如集成浏览、代码执行、插件等。相比之下，使用 OpenAI 的 API 提供了更大的灵活性，但需要您以编程方式编写代码或将请求发送到我们的模型
		- 以上知识点来源/值得反复看：查看链接[重磅发布！AI 驱动的 Java 开发框架：Spring AI Alibaba (qq.com)](https://mp.weixin.qq.com/s/1hRVYkqaBmKpBPV5V0E-Aw)
			- 我们在技术上有三个技术选型 ：Prompt / RAG / 微调。
				- Prompt：[[#red]]==效果略有提升==，但是不能带来本质改变；
				- 微调：[[#red]]==成本比较高==，我们的数据还不断迭代过程中无法承受；
				- RAG：无论是成本、效果，还是可持续迭代性，都是[[#red]]==目前最高投入产出比==模式，因此我们采用了 RAG 为主的技术方案。
			- **AI 答疑专家-实践**
				- AI 答疑专家基于百炼的通义 2.5 模型，将[[#red]]==开源文档、电子书、常见问题==灌入百炼数据中心，进行了向量化；[[#red]]==通过 Spring AI Alibaba 对接通义模型和 RAG 能力==，搜索到了 TOP3 的相关度信息，进行压缩提炼。并通过 Higress 将服务发布到开源官网和钉钉机器人，在入口构建安全合规防线。最后通过 AI 答疑专家[[#red]]==不断与开发者沟通，收集反馈==。通过 Chat-Admin 处理反馈差的信息，补充文档，优化数据。
			- **OTel：提升大模型应用可观测性**
			- **Nacos：提升 Agent 灵活性**
			- **Apache RocketMQ：提升 AI 应用吞吐量和实时性**
			  是不是所有的业务都能用 AI 解决呢？目前看不是的。那 AI 适合做什么场景呢？目前看，适合**容错性高、结构化强**的场景。
	- 从人的智能到“双智协同”；从人类视角到Ai视角
	  collapsed:: true
		- 人类本质上是智力系统的竞争，否则为何一定要那么多人削尖脑袋读到博士呢？
		- 使用有一些门槛的工具；前提：工具本身要很好用！
		- 一旦艺术get到技术：会有很强的发散和收敛能力
	- 技术在跳跃性发展，过去一年产生的图片==人类过去150年
	- 电能转化为智能：可见的生产力
	  collapsed:: true
		- 全球最大规模的工业革命的国家 居然是中国
		- 当前的发达国家都是先进的工业体：都是先进生产力的代表
	- 数字素养：使用和善用数字的能力；产生数字能力
	  collapsed:: true
		- 数字素养的五力：
		  洞察力 研究力 探索力 交叉力
		  最后落地：数字技术力
		- • 数宇素养是指社会公民在学习、工作、生活中应具备的适应数字
		  化社会的一系列基本能力与素质。
		  • 公民为了有效参与数字化社会的发展，必须具备数字资源的使用
		  和创造能力。它既包括对数字资源的接收能力，也包括对数字资
		  源的给予能力。
		- 数字素养在全球范围内不同国家的诠释
		  collapsed:: true
			- 欧盟：数字素养的不同诠释
			  欧盟发布的《欧盟公民数字素养框架》2.2
			  版将数字素养分为信息和数据、沟通与合
			  作、数字内容创作、安全、问题解决等五
			  个维度。
			- 美国数字素养平台和維基百科对数字素养的定义：
			  1利用数字技术，通讯工具或网络查找、评估、使用与
			  创造信息的能力； （使用与创造信息的能力）
			  2.理解和使用通过计算机呈观的多信源、多格式信息的
			  能力：（处理不同信息的能力）
			  3.在数字环境中有效完成任务的能力，包括阅读与理解
			  媒体、借助數字技术再观數据和團像以及評估和应用从
			  数字环境中获得的新知识的能力。（過过数据有效完成
			  任努的能力）
			- 数字素养的不同诠释
			  中央网络安全和信息化委员会印发的《提升全民数字素养
			  与技能行动纲要2022~2035》指出：数字素养是数字社会
			  公民学习工作生活应具备的数字获取、制作、使用、评
			  价、交互、分享、创新、安全保障、伦理道德等一系列素
			  质与能力的集合。
			  数字素养能力横型包括：数字生存能力、数宇安全能力、
			  字思维旅力、数字生产能力、欧字创新能力。
	- 数字化和智能化时代，什么样的能力才是强能力？
	  collapsed:: true
		- 学习之后超好用，有门槛的能力才是强能力；
	- 5个范式；第5范式
	  collapsed:: true
		- 第四/第五范式下设计研究的特征：
		  • 数据成为研究客体的主体
		  设计研究的客体不事局限于物，人，空间的物理形态与关系，而澤步本里关注其常后的熬据，魏業也不更仅仅是设
		  计研究的结果，而是变成了设计研究的基础，可以通过对于海是数握的挖握与分析，揭示数据中的有用价值，探求
		  其反肤的物理世界，如消费者喜好、产品性能属性、人机关系等等。
		  • 研究范式转为数据驱动
		  大教据时代，设计研究正由传统的问琴幫动，假沒驱动转向數響驱动。大數無弥补了传統研究中数紧量不足的问题。
		  更为研究者提供了发现问题、获取知识的途径。
		  设计研究者的核心主导作用将弱化
		  在传统设计研究领城，设计研究者帮着核心主异作息，从发现同题。分析向题和幫决早酸都定全需票研究者都力的
		  参与，第四/第五范式下，基于计算机主动运算的数据挖悍、 处理 分析，及人工智能介入的问题处理，将成为了设
		  计研究話动不可缺少的組成部分。'人智+肌智”的惯式成为设计的主要形式。
	- 沉浸式体验/沉浸式集成；
	  collapsed:: true
		- ________ Is All You Need;Attention Is All You Need
		- 科技是怪兽或天使 取决于体验
		- 专业人员的最佳实践 ，省去盲目摸索，不仅痛苦，而且没结果！
- ## 具体展开
	- 工作流
	  collapsed:: true
		- 比如做抖音视频文案（3分钟搞定）
			- 先写清楚需求（可以给一个需求框架）
			- 自动生成抖音文案；无限生成，供您选择；
			- 选择后插入；继续润色/改写；
	- 工作效能影响因素
	  collapsed:: true
		- 人员技能：google的工程师，号称10倍工程师，一个抵10个，实现全栈功能-----全栈能力
			- 初级工程师如何秒变高级工程师？
		- 协同消耗：项目架构和组织复杂度正相关，并决定协同消耗大小-----超级个体
			- 一次跳转，会导致大量资源损耗
			- 互联网中，页面跳转，跳转率导致的流量损耗巨大
		- 成本控制：成本是效能优化的目的，同时也是约束条件-----工具赋能，事务性工作替代
	- 工作挑战&智能化机会&“程序猿们”如何处理（同构性思考&通过“他们”来照镜子）
	  collapsed:: true
		- 个体效率：工作人员重复性工作，简单工作，沟通的工作特别多，浪费时间。
		  collapsed:: true
			- 程序猿们是这样的：GPT Copilot and Agent提升研发的一致性
		- 研发管理流程化，缺乏灵活性，组织容易产生效率竖井（局部优化；信息孤岛；目标冲突；资源浪费），响应能力弱。
		  collapsed:: true
			- LLM 简化流程 提升应对可变性能力
		- 研发体验：现有工具散乱（百模大战，莫衷一是），操作不统一，学习成本高，切换代价大。
		  collapsed:: true
			- 对话方式，统一入口，降低研发的复杂度
		- 数字资产：研发知识缺乏沉淀，资产价值没有发挥出来。大部分都是负债，资产积累少。
		  collapsed:: true
			- SFT, RAG 增强 LLM隐性知识显性化
	- 大模型的一个趋势不可逆转----edge化，内嵌化
	  collapsed:: true
		- 大模型只是“脑”，缺少“腿”；
		- 好的应用远比好的模型更实用
		- 如何让聪明的大脑贴心的为我们服务才是第一位的！
		-
	- 人工智能的人机协同模式：
	  collapsed:: true
		- LLM as Copilot：
			- 人机对话能力；解决单点事务性工作效率问题
			- 工具：专业增强
			- 人：见多识广，提升Prompt能力
		- LLM as Agent ：
			- 自主完成任务能力；解决复杂任务协同效率问题
			- 工具：多领城专家
			  人：给定上下文，完成知识对齐
		- LLM as Facilitator：
			- 跨领域复合型能力；解决信息整合，分析，决策问问题
			- 工具：跨领城会诊
			- 人：负责创意、纠偏
	- Copilot模式优势
	  collapsed:: true
		- 解决小任务：上下文宽度限制
		- 人工确认采纳：模型幻觉问题
		- 高频次：准确性有限
		- 短输出：推理成本与性能
	- 多个大模型比较
		- | LLM比较 | 文心一言 | 通义 | chatGLM | kimi | perpexity | bing |
		  | 解释性 | 很好；详细且分段格式严格 | 中等偏上 |  |  | 中等 |  |
		  | 推理性 |  | 很好 | 好 |  |  |  |
		  | 搜索性 | 中等偏上 |  |  |  | 很好 | 很好 |
		  | Coding性 |  | 很好 | 中等偏上 | 中等 |  |  |
		  | 长文本 |  | 一般 |  | 非常好 |  |  |
		  | 人设 | 知心大姐 | 理工男 |  |  |  |  |
			- {{renderer :luckysheet, workbook@1727313356289}}
			-
			-
			-
	- 开发者最喜欢的Copilot
	  collapsed:: true
		- 高频刚需
			- 补全；问答；
			- 软文；短视频文案；
			- 润色；
			- 营销市场分析（知己知彼）；
		- 触手可及
			- 程序员们：沉浸式编程体验
			- 过去：问周边人；通过“搜索”去查，选择判断
			- 现在：高精度；实时；沉浸式工作体验；会话管理：直接插入或update；场景化切换：一键切换
		- 知我所想
			- 基于语义理解的自适应生成粒度决策
			- 基于输入信息的语义理解，充分让模型（OpenAI）理解不同场景下所需的生成粒度，从而让模型能够根据当前正在编写的位置，自适应决定应该生成的信息粒度
			- 当前页面/块 检索增强：根据当前页面/块提供的背景知识 进行检索增强
		- 唯我专属
	- 人工智能时代，知识传递形态发生巨大改变；根智慧+根知识；
	  collapsed:: true
		- 从口口相传到智能问答+智能查询
		- LLM智能大脑：自定义prompt，RAG检索增强，Agent平台
		- 智能工具：个人助理，智能洞察，智能问答，结构化处理
		- 笔记体系：数字化资产储存，随处可查，条理化知识体系
	- 金句
	  collapsed:: true
		- 新技术代表新生产力，说了这么多年，但大模型本身就是生产力的代表
		- 短期乐观；长期悲观；
		-
- ## 基础知识
	- 指令（Instructions）和提示（Prompts）（文心一言 解读）
		- 定义与功能（[[#red]]==指令和提示的典型示例==）
			- **指令（Instructions）**：通常更为详细和全面，它们不仅包含了具体的任务要求，还可能包括任务的背景、目标、步骤以及期望的输出格式等[[#red]]==。指令的目的==是[[#red]]==确保模型能够准确地理解任务，并按照预定的方式进行操作。==
				- ```
				  任务：撰写一篇关于“人工智能在医疗领域的应用”的文章。  
				    
				  背景：随着科技的飞速发展，人工智能（AI）正在逐渐渗透到各个行业，其中医疗领域是受益最为显著的领域之一。  
				    
				  目标：文章应详细介绍人工智能在医疗领域的几种主要应用，包括但不限于疾病诊断、个性化治疗方案制定、药物研发以及患者健康管理等方面。  
				    
				  步骤：  
				  1. 简要介绍人工智能的概念及其在医疗领域的重要性。  
				  2. 分别阐述上述提到的几种主要应用，每种应用都应包含具体的案例或研究成果作为支撑。  
				  3. 分析人工智能在医疗领域应用的优势和面临的挑战。  
				  4. 展望人工智能在未来医疗领域的发展前景。  
				    
				  期望的输出格式：文章应为结构清晰、逻辑严密的议论文，字数不少于1000字。
				  ```
			- **提示（Prompts）**：则相对简短和直接，它们[[#green]]==主要用于激发模型的某种特定响应或行为==。提示通常以一个或多个句子、短语或关键词的形式出现，它们[[#red]]==可以引导模型==生成特定的文本、图像或其他类型的输出。
				- ```
				  “人工智能正在改变医疗领域，请列举一种AI在医疗中的具体应用。”
				  “想象一下，一位医生使用AI辅助进行疾病诊断，描述这个场景。”
				  ```
		- 使用场景
		  collapsed:: true
			- **指令**：[[#red]]==更适用于复杂或需要精确控制的任务场景==。通过提供详细的指令，用户可以确保模型在完成任务时能够[[#red]]==遵循特定的规则和流程==。
			- **提示**：则更[[#red]]==常用于生成式任务中==，如文本创作、图像生成等。通过巧妙的提示，用户可以激发模型的创造力和想象力，生成丰富多样的内容。
		- 灵活性与创造性
		  collapsed:: true
			- **提示**：在灵活性和创造性方面通常更具优势。由于提示[[_green]]==通常较为简短且开放==，它们为模型提供了更多的自由度和发挥空间，使得生成的内容更加多样化和富有创意。
			- **指令**：虽然也具有一定的灵活性，但相对于提示来说，它们往往[[#red]]==更加严格和具体==，这[[#green]]==在一定程度上限制了模型的创造性==。
		- 对模型的影响
		  collapsed:: true
			- **指令**：[[#red]]==对模型的影响更为直接和显著==。由于[[_green]]==指令包含了任务的所有关键信息==，因此它们[[#red]]==能够直接引导==模型的行为和输出。
			- **提示**：对模型的影响则可能更加[[#red]]==微妙和间接==。提示主要[[_green]]==通过激发模型的某种特定响应或行为来间接影响模型==的输出。
		- 实际应用
		  collapsed:: true
			- 在实际应用中，指令和提示往往是相辅相成的。用户可以[[#red]]==根据任务的具体需求选择合适的指令和提示组合来引导模型的行为==。例如，在编写一篇科技文章时，用户可以[[#green]]==先给出详细的指令来定义文章的主题、结构和要点，==然后再通过[[#green]]==具体的提示来激发模型的创造力和想象力，==生成丰富多样的内容。
		- 综合
		  collapsed:: true
			- 综上所述，指令和提示在大模型的应用中各有其独特的作用和优势。它们共同构成了用户与模型之间互动的桥梁，使得用户能够更加灵活和高效地利用大模型来完成各种任务。
		- 两个典型的问题
		  collapsed:: true
			- 问题1：如果以前述指令的格式输入到通义或文心中，但事实上因为是浏览器端的提问，而不是在大模型端的设置，所以它虽然是指令格式，依然被认为是提示？只是这个提示的长短和深度更好而已？
			  collapsed:: true
				- 是的，您的理解是正确的。在浏览器端向通义千言、文心一言等大型语言模型提问时，无论您输入的是长指令还是短提示，[[#green]]==从模型的角度来看，它们都被视为激发模型产生响应的“提示”==。这是因为用户是在与模型的接口（即浏览器页面）进行交互，而不是在模型内部进行配置或设置。
				  不过，就像您提到的，[[#red]]==这些“提示”在长度、详细程度和深度上==可能会有所不同，从而影响模型生成内容的性质和质量。
				  [[_green]]==长指令（即更详细的提示）通常包含更多的背景信息、目标要求和期望输出格式==，这有助于模型更好地理解任务并生成更符合要求、结构更清晰、内容更丰富的输出。相比之下，简短的提示可能只激发模型产生某种直接的、精炼的响应。
				  因此，尽管这些输入在技术上都是“提示”，但用户[[_green]]==可以通过调整提示的内容和形式来影响模型的输出，使其更符合自己的需求和期望==。在实际应用中，用户应该根据任务的具体要求和目标来选择合适的提示方式。
			- 问题2:根据这两个示例：
			  collapsed:: true
			  ```
			  示例1:
			  {"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "Paris, as if everyone doesn't know that already."}]}
			  示例2:
			  {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
			  ```
			  
			  来回答：是不是本质都是提示型而非指令型，无非是示例1把提示的角色做了设定的对话格式，示例2把提示的对话格式做了设定？（实际上就是简单的问答对格式）
				- 您的理解是基本正确的，但我们可以稍微澄清一下“提示型”和“指令型”的区别，并进一步明确这两个示例的特点。
				  
				  在机器学习和自然语言处理中，“提示型”（Prompt-based）和“指令型”（Instruction-based）方法有时被用来描述不同类型的任务设置或训练策略。然而，这两个术语并不是严格定义的，并且它们的含义可能因上下文而异。
				  
				  提示型（Prompt-based）：这种方法通常涉及[[#red]]==给模型提供一个“提示”（prompt），即一个文本片段或问题，模型需要根据这个提示来生成或选择响应==。提示可以是简单的（如单个句子或问题），也可以是复杂的（如包含多个句子或上下文信息的对话）。在您的示例中，无论是示例1的对话格式还是示例2的提示完成对格式，都可以看作是提示型方法的应用，因为模型都是[[_green]]==根据给定的提示（无论是对话中的用户消息还是单独的文本提示）来生成响应==的。
				  指令型（Instruction-based）：这个术语[[#green]]==通常与更高级别的任务描述或指令相关==联，这些描述或指令[[#red]]==不仅提供了生成响应所需的文本提示，还可能包含关于任务性质、目标或期望行为的更广泛信息。==在一些研究中，指令型方法被用来训练模型执行更复杂的任务，这些任务可能需要模型理解并遵循多步骤的指令。然而，在您的示例中，并没有直接涉及到复杂的指令型设置；示例1和示例2更多地是关注于提供不同类型的提示，以便模型能够生成相应的响应。
				  现在，让我们更具体地讨论一下这两个示例：
				  
				  示例1：这个示例采用了对话格式，其中[[#red]]==包含了多个角色（系统、用户、助手）和消息==。在这个格式中，[[_green]]==系统消息可以被视为一种“提示”，它设定了对话的上下文和期望的行为==（即助手应该以讽刺的方式回应）。然而，与简单的文本提示不同，[[#green]]==这个“提示”是一个包含角色和对话历史的复杂结构==。尽管如此，从广义上讲，它[[_green]]==仍然可以被视为一种提示型方法的应用。==
				  示例2：这个示例采用了提示完成对格式，即每个示例由一个提示和一个完成组成。在这个格式中，[[_green]]==提示是一个简单的文本片段或问题，而完成则是期望的响应==。这种格式更接近于传统的问答对或文本生成任务，其中模型只需要根据给定的提示来生成响应。因此，从这个角度来看，示例2更直接地体现了提示型方法的特点。
				  综上所述，虽然这两个示例都可以被视为提示型方法的应用，但它们在提示的具体形式和结构上有所不同。示例1[[#red]]==通过对话格式提供了更丰富的上下文信息==，而示例2则[[#red]]==采用了更简单的问答对格式。==
-
- ## 官方文档 https://platform.openai.com/docs精读
	- 提示词工程prompt-engineering；获得更好结果的 6 种策略
	  collapsed:: true
		- 网址：(https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results)（该链接中有详细示例与具体拆解，值得反复看）
		- 编写清晰的说明
		- 提供参考文本
		- 将复杂任务拆分为更简单的子任务
		- 给模型时间“思考”
		- 使用外部工具
		- 系统地测试更改
	- 文本生成模型text-generation-models；大模型型号选择： gpt-4o-mini；
	  collapsed:: true
		- (https://platform.openai.com/docs/guides/text-generation/text-generation-models)
		- 聊天完成 (chat.completion)API 调用示例
		  collapsed:: true
			- ```
			  from openai import OpenAI
			  client = OpenAI()
			  response = client.chat.completions.create(
			  model="gpt-4o-mini",
			  messages=[
			    {"role": "system", "content": "You are a helpful assistant."},
			    {"role": "user", "content": "What is a LLM?"}
			  ]
			  )
			  ```
		- 型号选择：我们通常建议您默认使用gpt-4o或 gpt-4o-mini；我们希望在性能、成本和易用性方面成为大多数用户的正确模型：`gpt-4o-mini`
		  collapsed:: true
			- 如果您的使用案例需要高智能或对图像和文本进行推理，我们建议您同时评估 `gpt-4o` 和 `gpt-4-turbo`（尽管它们具有非常相似的智能，但请注意，[[#red]]==`gpt-4o` 更快且更便==宜）。
			- 如果您的使用案例需要[[#red]]==最快的速度和最低的成本==，我们建议使用 `gpt-4o-mini`，因为它针对这些方面进行了优化。
			- 我们建议您在[[#red]]==以前使用 `gpt-3.5-turbo` 的地方使用 `gpt-4o-mini`==，因为它更便宜、智能更高、上下文窗口更大（多达 128,000 个令牌，而 `gpt-3.5-turbo` 为 4,096 个令牌），并且是多模式的。
			- 您可以在 [[#red]]==[Playground]==(https://platform.openai.com/playground?mode=chat) 中进行实验，以研究哪些模型为您的使用提供了最佳的性价比权衡。一种常见的设计模式是使用几种不同的查询类型，每种类型都被分派给适合处理它们的模型。
		- （VIP用户角度）选择gpt-4o或 gpt-4o-mini的核心区别（启发：可以设置：更精确更多样化；更快更便宜）
		  id:: 66f3f99f-f8f8-4b09-9a7a-211952c539eb
		  collapsed:: true
			- https://platform.openai.com/docs/guides/production-best-practices/improving-latencies
			- 我们的 API 提供了具有不同复杂程度和通用性的不同模型。功能最强大的模型（如 `gpt-4`）可以[[#red]]==生成更复杂和多样化的补全==，但它们[[_green]]==也需要更长的时间==来处理您的查询。`gpt-4o-mini` 等模型可以[[#red]]==生成更快、更便宜==的聊天完成，但它们[[#red]]==生成的结果可能会不太准确或与您的查询相关==。您可以选择最适合您的使用案例的模型，并在[[#green]]==速度、成本和质量==之间进行权衡。
		- 我应该如何设置 temperature 参数？
		  collapsed:: true
			- (https://platform.openai.com/docs/guides/text-generation/how-should-i-set-the-temperature-parameter)
			- 您可以将温度视为*随机性*，其中 0 表示*最不随机*（或*最具确定性*） 2 表示*最随机*（*确定性最低*）。当使用较低的温度值（例如 0.2）时， 模型响应往往更加一致，但可能会感觉更机器人。大于 1.0 的值， 尤其是接近 2.0 的值，可能会导致模型输出不稳定。如果您的目标是广告素材 输出，即[[#red]]==略高于正常温度（例如 1.2）==与提示符的组合 明确要求模型具有创造力可能是您最好的选择，但我们鼓励实验。
	- **使用长上下文扩展提示工程** “[[_green]]==Lost in the middle==” /“迷失在中间”；“[[_green]]==正确的基线==”
	  collapsed:: true
		- [Optimizing LLM Accuracy - OpenAI API](https://platform.openai.com/docs/guides/optimizing-llm-accuracy/llm-optimization-context)
		- 快速工程的最大问题是它通常无法扩展 - 我们要么需要[[_green]]==动态上下文==来允许模型处理比我们通过简单的上下文填充处理的更广泛的问题，要么我们需要[[_green]]==比使用少数样本实现的更一致==的行为
		- 长上下文模型允许提示工程进一步扩展 - 但是，请注意，模型可能难以在具有复杂指令的非常大的提示中保持注意力，因此您应该始终将长上下文模型与不同上下文大小的评估配对，以确保您不会[[_green]]==[**迷失在中间**](https://arxiv.org/==abs/2307.03172)。“Lost in the middle” 是一个术语，指的是 LLM 在任何时候都不能对给予它的所有代币给予同等的关注。这可能会导致它似乎随机地丢失信息。这并不意味着您不应该使用长上下文，但您需要将其与全面评估配对。
		  一位开源贡献者 Greg Kamradt 进行了一项名为 [**Needle in A Haystack （NITA）**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) 的有用评估，该评估将一条信息隐藏在长上下文文档中的不同深度，并评估检索质量。这说明了长上下文的问题 - 它承诺了一个更简单的检索过程，您可以在其中转储上下文中的所有内容，但以牺牲准确性为代价。
		- 这就是为什么[[#red]]==**带有一组评估问题和真实答案的好提示**==是这个阶段的最佳输出。如果我们有一组 20+ 个问题和答案，并且我们已经研究了失败的细节并假设了它们发生的原因，那么我们就有了[[$green]]==正确的基线来采用更高级的优化方法==。
		- 所以你已经完成了快速工程，你有一个 eval 集，但你的模型仍然没有做你需要它做的事情。最重要的下一步是[[#red]]==诊断它失败==的地方，以及什么工具最能改进它。
			- 您可以将每个失败的评估问题视为**[[_green]]==上下文问题==**或[[_green]]==**学习**性记忆==问题。
			  id:: 66f40743-c5f5-4414-a764-b019cf4614a4
			- 要么[[#red]]==以静态方式使用提示工程==，要么以[[#red]]==工业方式使用 RAG==。
	- 检索增强生成 （RAG）（链接中的框架图很重要！）（很多论断非常有价值）
	  id:: 66f407e0-55de-4e08-96f1-a979919d28bf
	  collapsed:: true
		- (https://platform.openai.com/docs/guides/optimizing-llm-accuracy/retrieval-augmented-generation-rag)
		- RAG 是在 **G**激活答案之前将内容检索到 **A**ugment 您的 LLM 提示的过程。它用于为模型提供**对特定于域的上下文的访问权限**以求解任务。
		- RAG 是提高 LLM 准确性和一致性的非常有价值的工具——我们在 OpenAI 的[[#red]]==许多最大客户部署都是**仅使用提示工程和 RAG 完成**的。==
	- 微调：为了解决[[#red]]==学习到的内存（To solve a learned memory problem,）==问题，许多开发人员将[[#red]]==在较小的特定领域数据集==上继续 LLM 的训练过程，以[[#red]]==针对特定任务==对其进行优化。称为**微调**
	  id:: 66f409aa-b8b4-43a6-b074-8f5020cfcee1
		- Part1：Optimizing LLM Accuracy - OpenAI API/docs指南之[[_green]]==精度优化==：微调的原因/过程/最佳实践
		  collapsed:: true
			- https://platform.openai.com/docs/guides/optimizing-llm-accuracy/understanding-the-tools
			- 执行微调通常出于以下两个[[#red]]==原因==之一：
			  id:: 66f409eb-1559-418f-b8ec-f9a17bc00f50
			  collapsed:: true
				- **要提高特定任务的模型准确性：**在特定于任务的数据上训练模型，通过向模型展示正确执行该任务的许多示例来解决学习到的内存问题。
				- **要提高模型效率，请执行以下操作：**使用较少的令牌或使用较小的模型实现相同的准确性。
			- 微调过程[[#red]]==从准备训练示例的数据集==开始 - 这是最关键的一步，因为您的微调示例必须准确表示模型在现实世界中看到的内容。
			- 在 OpenAI 的模型定制产品中观察到的[[#red]]==最佳实践==
			  collapsed:: true
				- **从 prompt-engineering 开始：**[[#red]]==从 prompt engineering 中获得一个可靠的评估集==，您可以将其用作[[_green]]==基线==。这允许采用低投资方法，直到您对基本提示有信心为止。
				- **从小处着手，注重质量：**在基础模型之上进行微调时，[[#red]]==训练数据的质量比数量更重要==。从 50+ 个示例开始，进行评估，如果您尚未达到准确性需求，并且导致错误答案的问题是由于一致性/行为而不是上下文，请调整您的训练集大小。
				- **[[_green]]==确保您的示例具有代表性==：**我们看到的最常见的陷阱之一是不具有代表性的训练数据，其中用于微调的示例在格式或形式上与 LLM 在生产中看到的示例略有不同。例如，如果您有一个 RAG 应用程序，请对包含 RAG 示例的模型进行微调，这样它就不会学习如何使用上下文 zero-shot。
			- 在生产解决方案中使用微调 + RAG的好处（见下下章“微调 + RAG”）
		- Part2：docs/guides/fine-tuning docs指南之[[_green]]==微调==：微调的价值（重要方向）/收益/步骤
			- https://platform.openai.com/docs/guides/fine-tuning/common-use-cases
			- 微调有效的另一种情况是，通过[[#red]]==将更昂贵的模型（如 `gpt-4o`）替换为微调的 `gpt-4o-mini` 模型来降低成本和/或延迟==。如果您可以使用 `gpt-4o` 获得良好的结果，则通常可以通过微调 `gpt-4o` 完成（可能缩短指令提示）来使用微调的 `gpt-4o-mini` 模型达到类似的质量。
			- 更多收益：Fine-tuning 允许您从 API 提供的模型中获得更多收益
			  collapsed:: true
				- 比提示[[_green]]==更高质量==的结果
				- 能够训练比提示所能[[_green]]==容纳的更多示例==
				- 由于提示[[_green]]==时间更短而节省的代币==
				- [[_green]]==更低的延迟请求==
			- 微调的步骤
			  collapsed:: true
				- 准备和上传训练数据
				- 训练新的微调模型
				- 评估结果并根据需要返回步骤 1
				- 使用微调后的模型
			- When to use fine-tuning /何时使用微调（原文：英文+中文）
			  collapsed:: true
				- Fine-tuning OpenAI text generation models can make them better for specific applications, but it requires a careful investment of time and effort. We recommend first attempting to get good results with prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and [function calling](https://platform.openai.com/docs/guides/function-calling), with the key reasons being:
				  微调 OpenAI 文本生成模型可以使它们更适合特定应用程序，但这需要仔细投入时间和精力。我们建议首先尝试通过提示工程、提示链接（将复杂任务分解为多个提示）和[函数调用](https://platform.openai.com/docs/guides/function-calling)来获得良好的结果，主要原因是：
				- There are many tasks at which our models may not initially appear to perform well, but results can be improved with the right prompts - thus fine-tuning may not be necessary
				  id:: 66f4f838-a0ba-43f2-8614-b7a8bfa19820
				  在许多任务中，我们的模型[[#red]]==最初可能表现不佳==，但可以通过正确的提示来改进结果 - 因此可能不需要微调
				- Iterating over prompts and other tactics has a much faster feedback loop than iterating with fine-tuning, which requires creating datasets and running training jobs
				  collapsed:: true
				  [[#red]]==迭代提示和其他策略的反馈循环==比使用[[#red]]==微调迭代==要快得多，后者需要创建数据集和运行训练作业
					- 迭代提示和其他策略的反馈循环
					  
					  “迭代提示和其他策略的反馈循环”指的是在模型开发和优化过程中[[_green]]==，不断尝试和调整提示（prompts）、提示链（即将复杂任务分解为多个提示）、函数调用等策略==，并根据模型的响应和性能反馈进行迭代改进的过程。这个循环旨在通过不断优化这些策略来提高模型在特定任务上的表现。在这个过程中，开发者会不[[#green]]==断尝试新的提示设计、提示组合或策略调整==，并观察模型输出的变化，以此为依据进行下一步的改进。
					- 迭代微调
					  
					  “迭代微调”则是指通过多次对模型进行微调（fine-tuning）来优化其在特定任务上的性能。在微调过程中，开发者会使用特定领域的数据集来训练模型，以使其更加适应这些数据。与迭代提示循环类似，[[#green]]==迭代微调也是一个反复尝试和调整的过程==。开发者可能会尝试[[_green]]==不同的微调参数、不同的训练数据或不同的训练策略==，并观察模型在验证集或测试集上的表现，以此为依据进行下一步的微调。
				- In cases where fine-tuning is still necessary, initial prompt engineering work is not wasted - we typically see best results when using a good prompt in the fine-tuning data (or combining prompt chaining / tool use with fine-tuning)
				  collapsed:: true
				  在仍然需要微调的情况下，初始提示工程工作不会浪费 - 在微调数据中[[#red]]==使用良好的提示（或将提示链/工具使用与微调相结合）==时，我们通常会看到最佳结果
					- 在某些情况下，尽管已经进行了提示工程等优化工作，但模型在特定任务上的表现仍然不尽如人意，此时微调可能成为必要的手段。然而，这并不意味着之前的提示工程工作是徒劳的。相反，开发者可以[[#green]]==将这些经过优化的提示、提示链或工具使用策略融入到微调过程中==，即[[_green]]==在微调数据集中使用这些好的提示，或者将提示链接/工具使用与微调相结合==。这样做可以[[#green]]==充分利用之前的工作成果==，进一步提高模型在微调后的性能表现。因此，即使需要微调，初始的提示工程工作也不会浪费，而是会为最终的模型优化提供有益的支持。
			- Common use cases /常见用例（原文：英文+中文；文新分段逐一解读）
			  collapsed:: true
				- Setting the style, tone, format, or other qualitative aspects
				  collapsed:: true
				  设置样式、语气、格式或其他定性方面
					- 在文本生成任务中，有时需要模型输出符合特定风格、语气或格式的文本。例如，在撰写新闻报道时，可能需要正式且客观的语气；而在创作广告文案时，则可能需要更加生动和吸引人的语言。微调可以帮助模型学习并适应这些定性要求，从而生成更符合期望的文本。
				- Improving reliability at producing a desired output
				  collapsed:: true
				  提高产生所需[[#red]]==输出的可靠性==
					- 对于某些任务，如自动问答或摘要生成，模型需要[[#green]]==可靠地产生准确且有用的输出==。然而，预训练模型可能由于[[_green]]==未完全适应特定领域或任务而偶尔出现错误==。通过微调，模型可以针对特定任务进行训练，从而提高其产生所需输出的可靠性。
				- Correcting failures to follow complex prompts
				  collapsed:: true
				  更正故障[[#red]]==以遵循复杂的提示==
					- 在某些情况下，模型[[#green]]==可能无法完全理解或遵循复杂的提示==，导致输出不符合预期。微调可以帮助模型更好地理解这些复杂提示，并学习如何根据这些提示产生正确的输出
				- Handling many edge cases in specific ways
				  collapsed:: true
				  以特定方式处理许多[[#red]]==边缘情况==
					- [[_green]]==边缘情况是指那些在数据集中不常见但可能在实际应用中出现的特殊情况==。预训练模型可能无法很好地处理这些边缘情况。通过微调，模型可以学习如何以特定方式处理这些边缘情况，从而提高其在复杂环境中的鲁棒性。
				- Performing a new skill or task that’s hard to articulate in a prompt
				  collapsed:: true
				  执行难以在提示中表达的[[#red]]==新技能或任务==
					- 有些新技能或任务可能[[#green]]==很难通过简单的提示==来传达给模型。例如，要求模型[[_green]]==编写一首符合特定主题和风格的诗歌==，或者[[_green]]==生成一段具有特定情感色彩的对话==。在这种情况下，微调可以帮助模型学习这些新技能或任务，并产生更符合期望的输出。
				- One high-level way to think about these cases is when it’s easier to "show, not tell". In the sections to come, we will explore how to set up data for fine-tuning and various examples where fine-tuning improves the performance over the baseline model.
				  collapsed:: true
				  思考这些案例的一种高级方法是“展示，而不是讲述”更容易。在接下来的章节中，我们将探讨如何设置用于微调的数据，以及微调可[[#red]]==提高基线模型性能==的各种示例。
					- 这些用例背后的一个高级思考方式是“展示，而不是讲述”。在微调过程中，我们不仅仅是告诉模型应该做什么（通过提示或任务描述），而是[[#green]]==直接向模型展示如何完成特定任务（通过提供标注好的训练数据）==。这种方式使得模型能够[[#green]]==更直观地理解任务要求==，并学习如何产生高质量的输出。
					- 在接下来的章节中，我们将深入探讨如何设置用于微调的数据集，以及通过具体示例展示微调如何帮助模型在多个领域和任务上实现性能提升
			- 微调之前：制作提示/Crafting prompts
			  collapsed:: true
				- 要点解读---文心
					- **保留最佳指令**：在微调前找到并保留对模型最有效的指令和提示集，并在每个训练示例中包含它们，以获得最佳和最通用的结果。
					- **[[#red]]==成本与效果的权衡==**：为了节省成本而缩短指令时，需意识到模型可能难以在推理时忽略这些“内置”指令。
					  collapsed:: true
						- （进一步提问文心的解释）
						  collapsed:: true
							- 在成本与效果的权衡中，是否应该缩短指令取决于多个因素，包括你的具体需求、资源限制以及模型应用的上下文。
							- **资源限制**：如果你的计算资源或时间非常有限，缩短指令可能是一个合理的选择，因为它可以减少训练过程中的计算成本。然而，你需要意识到这可能会牺牲一些模型性能。
							- **模型性能需求**：如果你的应用场景对模型性能有很高的要求，那么保留完整的指令可能更合适。这有助于模型在训练过程中学习到更精确、更通用的特征，从而提高推理时的准确性。
							- **模型的可解释性和可控性**：缩短指令可能会使模型的行为变得不那么可预测或可控。如果模型在推理时难以忽略这些“内置”指令，那么它可能会在某些情况下产生不符合预期的输出。这可能会对你的应用产生负面影响。
							- id:: 66f4f5c6-9925-4f03-a800-301a8112422d
							  
							  **实验和验证**：最好的方法是通过实验来验证不同指令长度对模型性能的影响。你可以尝试在不同的数据集和模型架构上训练模型，并比较不同指令长度下的性能表现。这将帮助你更好地理解缩短指令对模型性能的具体影响，并做出更明智的决策。
							  
							  综上所述，是否应该缩短指令取决于你的具体需求和资源限制。如果你有足够的资源并且对模型性能有很高的要求，那么保留完整的指令可能更合适。如果你的资源有限，并且可以接受一定程度的性能牺牲，那么缩短指令可能是一个可行的选择。然而，在任何情况下，你都应该进行实验和验证来确保你的决策是合理的。
						- （通义补充）如果为了节省成本而缩短或省略指令和提示，模型在推理时仍可能表现得好像这些指令存在一样，这可能导致难以控制模型的行为。
					- **训练示例的需求**：如果不包含完整指令，模型可能需要更多的训练示例来通过演示学习，因为缺少了额外的指导说明。
				- 原文（中英文解释）
				  collapsed:: true
					- We generally recommend taking the set of instructions and prompts that you found worked best for the model prior to fine-tuning, and including them in every training example. This should let you reach the best and most general results, especially if you have relatively few (e.g. under a hundred) training examples.
					  我们通常建议在微调之前采用您认为最适合模型的指令和提示集，并将它们包含在每个训练示例中。这应该可以让你达到最好和最通用的结果，特别是如果你的训练示例相对较少（例如，[[_green]]==少于一百个==）。
					- If you would like to shorten the instructions or prompts that are repeated in every example to save costs, keep in mind that the model will likely behave as if those instructions were included, and it may be hard to get the model to ignore those "baked-in" instructions at inference time.
					  如果您想缩短每个示例中重复的指令或提示以节省成本，请记住，模型的行为可能会像包含这些指令一样，并且可能很难让模型在推理时忽略这些“内置”指令。
					- It may take more training examples to arrive at good results, as the model has to learn entirely through demonstration and without guided instructions.
					  可能需要更多的训练示例才能获得良好的结果，因为模型必须完全通过演示进行学习，而无需指导说明。
			- Example count recommendations/计数建议示例
				- 要微调模型，您需要提供至少 10 个示例。使用 `gpt-4o-mini` 和 `gpt-3.5-turbo` 对 [[_green]]==50 到 100== 个训练示例进行微调后，我们通常会看到明显的改进，但正确的数量会因确切的用例而有很大差异。
				- 我们建议从[[#green]]==50 个精心制作的演示（well-crafted demonstrations）==开始，看看模型在微调后是否显示出改进的迹象。在某些情况下，这可能就足够了，但即使模型尚未达到生产质量，[[_green]]==明显的改进也是一个好兆头==，表明提供更多数据将继续改进模型。没有改进表明，在扩展到有限的示例集之前，您可能需要重新考虑如何为模型设置任务或重组数据。
			- Train and test splits/训练和测试拆分
				- 收集初始数据集后，我们建议将其==拆分为训练和测试部分==。在提交包含训练和测试文件的微调作业时，我们将在训练过程中[[_red]]==提供有关两者的统计信息==。这些统计数据将是模型改进程度的初始信号。此外，[[#green]]==尽早构建测试集==将有助于确保您能够在训练后通过在测试集上生成样本来评估模型。
			- Token limits/令牌限制
				- | 型 | 推理上下文长度 | 训练示例上下文长度 |
				  | ---- | ---- | ---- |
				  | `gpt-4o-2024-08-06` | 128,000 个代币 | 65,536 个代币 （128k 即将推出） |
				  | `gpt-4o-mini-2024-07-18` | 128,000 个代币 | 65,536 个代币 （128k 即将推出） |
			-
		- Part3：微调API（见notion相关章节整理的命令系列）[API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/fine-tuning/create)
	- 微调 + RAG（LLM 优化矩阵/操作流程/好处）：调整准确性可能是一场与 LLM 的永无止境的战斗；文科生；
		- ![准确性心智模型图](https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-01.png)
		- 典型的 LLM 任务将从左下角开始，提示工程，我们在其中测试、学习和评估以获得基线。一旦我们审查了这些基线示例并评估了它们为什么不正确，我们就可以拉动我们的杠杆之一：
		  collapsed:: true
			- **上下文优化：**在以下情况下，您需要针对上下文进行优化：1） 模型[[_green]]==缺乏上下文知识==，因为它不在训练集中，2） 它的[[_green]]==知识已过时==，或者 3） 它需要[[_green]]==专有信息的知识==。该轴最大限度地提高**了[[#red]]==响应精度==**。
			- **LLM 优化：**在以下情况下，您需要优化 LLM：1） 模型产生不一致的结果，[[_green]]==格式不正确==，2） [[_green]]==语气或语音风格==不正确，或者 3） [[#green]]==推理没有得到一致遵循==。此轴可最大程度地提高**[[#red]]==行为的一致性==**。
			- ![准确性心智模型旅程图](https://openaiassets.blob.core.windows.net/$web/API/docs/images/diagram-optimizing-accuracy-02.png)
		- 在此示例中，我们执行以下操作：（微调的操作流程/含RAG）
		  collapsed:: true
			- 从提示开始，然后[[_green]]==评估其性能==
			- 添加[[_green]]==静态的 few-shot 示例==，这应该会[[_green]]==提高结果的一致性==
			- [[_green]]==添加检索步骤==，以便根据问题[[_green]]==动态引入少数样本== - 这可以通过[[_green]]==确保每个输入的相关上下文来==提高性能
			- 准备包含 [[#green]]==50+ 个示例的数据集==并[[#red]]==微调模型==以提高一致性
			- [[_green]]==调整检索==并[[_green]]==添加事实核查步骤以查找幻觉==，以实现更高的准确性
			- [[#green]]==在新的训练示例上重新训练微调的模型==，其中[[#green]]==包括我们增强的 RAG 输入==
			  id:: 66f4a721-a60e-4859-8114-fbae13e3cc59
			  
			  这是一个针对棘手业务问题的[[#red]]==相当典型的优化管道== - 它帮助我们确定我们[[_green]]==是否需要更相关的上下文==，或者我们[[_green]]==是否需要来自模型的更一致的行为==。一旦我们做出了这个决定，我们就知道该拉哪个杠杆作为我们迈向优化的第一步。
		- 在生产解决方案中使用微调 + RAG的好处
		  collapsed:: true
			- 使用微调来[[#green]]==最大限度地减少用于提示工程的标==[[#green]]==记==，因为您[[_green]]==将指令和小样本替换为许多训练示例==，以在模型中根深蒂固一致的行为。
			- 使用广泛的微调来[[#green]]==教授复杂行为==
			- 使用 RAG **注入上下文**、更新的内容或您的用例所需的任何其他[[#red]]==专用上下文==
		- 最后一件事是，一旦您引入了它们，我们的[[#red]]==迭代速度就会受到权衡==：
		  id:: 66f40c9c-29fc-45c9-b539-5663a493211a
		  collapsed:: true
			- 对于 RAG，您需要[[_green]]==调整检索以及 LLM 行为==
			- 使用 fine-tuning 时，您需要[[_green]]==重新运行微调过程==，并在[[_green]]==执行其他优化时管理您的训练集和验证集==。
		- 注意事项：这两者都可能是耗时且复杂的过程，随着 LLM 应用程序变得更加复杂，这可能会引入回归问题。如果您从本文中学到一件事，那就是在进行更复杂的 RAG 或微调之前，尽可能多地[[_green]]==从基本方法中挤出准确性== - [[#red]]==让您的精度目标成为目标==，而不是跳转到 RAG + FT，因为它们被认为是最复杂的。
		- 对于生产来说，多少精度是“足够好的”：调整准确性可能是一场与 LLM 的永无止境的战斗 - 使用现成的方法，它们不太可能达到 99.999% 的准确率。
-
- 参考链接：
	- {{video https://www.bilibili.com/video/BV1LH4y1V7Y7/?spm_id_from=333.788&vd_source=e13b46e9b43415aab34a87e9281703c7}}